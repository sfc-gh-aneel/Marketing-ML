{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training & Registry with Snowflake ML\n",
        "## Financial Services ML Pipeline - Native Snowflake Implementation\n",
        "\n",
        "This notebook demonstrates model training and registration using Snowflake's Model Registry for financial services ML.\n",
        "\n",
        "## What We'll Build\n",
        "- **Classification Models**: Conversion prediction, churn prediction\n",
        "- **Multi-class Classification**: Next best action recommendation\n",
        "- **Model Comparison**: XGBoost, Random Forest, and LogisticRegression\n",
        "- **Model Registry**: Version control and lifecycle management\n",
        "- **Performance Evaluation**: Comprehensive model assessment\n",
        "\n",
        "## Snowflake ML Features Used\n",
        "- **Snowpark ML**: Native ML training within Snowflake\n",
        "- **Model Registry**: Centralized model management and versioning\n",
        "- **Cross-validation**: Robust model evaluation\n",
        "- **Feature Engineering**: Automated preprocessing pipelines\n",
        "- **Model Deployment**: Seamless deployment for inference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ö†Ô∏è Important: Database Configuration\n",
        "\n",
        "**Before running this notebook:**\n",
        "\n",
        "1. **If you DID NOT run Cell 11** in Feature Engineering:\n",
        "   - This notebook will automatically use `FINANCIAL_ML_DB`\n",
        "   - No changes needed\n",
        "\n",
        "2. **If you DID run Cell 11** in Feature Engineering:\n",
        "   - You created a new database with a timestamp (e.g., `FINANCIAL_ML_DEMO_20250923_143052`)\n",
        "   - Update Cell 2 below with your database name\n",
        "   - Look for the commented lines to uncomment and update\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries for ML training\n",
        "import snowflake.snowpark as snowpark\n",
        "from snowflake.snowpark import Session\n",
        "from snowflake.snowpark.functions import *\n",
        "from snowflake.snowpark.window import Window\n",
        "from snowflake.snowpark.functions import row_number, lit\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "# Import Snowflake ML modules with version compatibility\n",
        "try:\n",
        "    # Try new import structure first\n",
        "    from snowflake.ml.modeling.xgboost import XGBClassifier\n",
        "    from snowflake.ml.modeling.ensemble import RandomForestClassifier\n",
        "    from snowflake.ml.modeling.linear_model import LogisticRegression\n",
        "    from snowflake.ml.modeling.preprocessing import StandardScaler, LabelEncoder\n",
        "    print(\"‚úÖ Snowflake ML modeling imports successful\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è Using alternative import structure\")\n",
        "    from snowflake.ml.modeling import xgboost, ensemble, linear_model, preprocessing\n",
        "    XGBClassifier = xgboost.XGBClassifier\n",
        "    RandomForestClassifier = ensemble.RandomForestClassifier\n",
        "    LogisticRegression = linear_model.LogisticRegression\n",
        "    StandardScaler = preprocessing.StandardScaler\n",
        "    LabelEncoder = preprocessing.LabelEncoder\n",
        "\n",
        "# Import train_test_split with fallback\n",
        "try:\n",
        "    from snowflake.ml.modeling.preprocessing import train_test_split\n",
        "    print(\"‚úÖ train_test_split imported from preprocessing\")\n",
        "except ImportError:\n",
        "    try:\n",
        "        from snowflake.ml.modeling.model_selection import train_test_split\n",
        "        print(\"‚úÖ train_test_split imported from model_selection\")\n",
        "    except ImportError:\n",
        "        # Fallback: manual train/test split\n",
        "        print(\"‚ö†Ô∏è train_test_split not available, using alternative approach\")\n",
        "        print(\"   Note: For production use, ensure Snowflake ML is properly installed\")\n",
        "        \n",
        "        # Alternative: Use Snowpark's sample method for splitting\n",
        "        from snowflake.snowpark.dataframe import DataFrame\n",
        "        \n",
        "        # Override train_test_split to None so we can handle it differently\n",
        "        train_test_split = None\n",
        "\n",
        "# Import metrics\n",
        "try:\n",
        "    from snowflake.ml.modeling.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "    print(\"‚úÖ Metrics imported successfully\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è Some metrics may not be available\")\n",
        "\n",
        "# Import Registry\n",
        "try:\n",
        "    from snowflake.ml.registry import Registry\n",
        "    print(\"‚úÖ Model Registry imported successfully\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è Model Registry not available\")\n",
        "    Registry = None\n",
        "\n",
        "# Get active session\n",
        "session = snowpark.session._get_active_session()\n",
        "\n",
        "print(f\"ü§ñ Snowflake ML Model Training Pipeline\")\n",
        "print(f\"Database: {session.get_current_database()}\")\n",
        "print(f\"Schema: {session.get_current_schema()}\")\n",
        "print(f\"Warehouse: {session.get_current_warehouse()}\")\n",
        "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Set up correct database/schema based on Feature Engineering approach\n",
        "try:\n",
        "    # Try original database first\n",
        "    session.sql(\"USE DATABASE FINANCIAL_ML_DB\").collect()\n",
        "    session.sql(\"USE SCHEMA ML_PIPELINE\").collect()\n",
        "    test_count = session.sql(\"SELECT COUNT(*) FROM FEATURE_STORE\").collect()[0][0]\n",
        "    print(f\"\\n‚úÖ Using database: FINANCIAL_ML_DB\")\n",
        "except:\n",
        "    # If original fails, use the Feature Store database\n",
        "    print(\"\\n‚ö†Ô∏è FINANCIAL_ML_DB not accessible, using Feature Store database\")\n",
        "    session.sql(\"USE DATABASE FINANCIAL_ML_DEMO_20250923_093605\").collect()\n",
        "    session.sql(\"USE SCHEMA ML_PIPELINE\").collect()\n",
        "    print(\"‚úÖ Using database: FINANCIAL_ML_DEMO_20250923_093605\")\n",
        "\n",
        "# Verify feature store availability\n",
        "fs_count = session.sql(\"SELECT COUNT(*) as count FROM FEATURE_STORE\").collect()[0]['COUNT']\n",
        "feature_count = session.sql(\"SELECT COUNT(*) as feature_count FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'FEATURE_STORE'\").collect()[0]['FEATURE_COUNT']\n",
        "\n",
        "print(f\"\\nFeature Store Ready:\")\n",
        "print(f\"üìä Training Records: {fs_count:,}\")\n",
        "print(f\"üîß Available Features: {feature_count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional: Use Feature Store API\n",
        "\n",
        "If you registered your features in the Feature Store UI (Cell 11 in Feature Engineering), you can optionally use the Feature Store API instead of direct table access. Skip this cell if you prefer the simpler direct table approach.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative: Simpler Feature Store retrieval\n",
        "# Uncomment this cell if you want a simpler approach to using the Feature Store\n",
        "\n",
        "\"\"\"\n",
        "# Simple Feature Store data retrieval\n",
        "from snowflake.ml.feature_store import FeatureStore\n",
        "\n",
        "# Initialize Feature Store\n",
        "fs = FeatureStore(\n",
        "    session=session,\n",
        "    database=session.get_current_database(),\n",
        "    name=\"FEATURE_STORE\",\n",
        "    default_warehouse=session.get_current_warehouse()\n",
        ")\n",
        "\n",
        "# Simply retrieve the entire feature table\n",
        "# This is often more straightforward than using feature views\n",
        "training_df = session.table(\"FEATURE_STORE\")\n",
        "\n",
        "print(f\"‚úÖ Retrieved {training_df.count()} records from Feature Store\")\n",
        "print(f\"   Columns: {len(training_df.columns)}\")\n",
        "\n",
        "# You can also retrieve specific features for specific entities\n",
        "# Example: Get features for specific clients\n",
        "specific_clients_df = training_df.filter(col(\"CLIENT_ID\").in_([1, 2, 3, 4, 5]))\n",
        "print(f\"   Sample: {specific_clients_df.count()} records for specific clients\")\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OPTIONAL: Use Feature Store API (skip if using direct table access)\n",
        "USE_FEATURE_STORE_API = False  # Set to True if you want to use Feature Store API\n",
        "\n",
        "if USE_FEATURE_STORE_API:\n",
        "    try:\n",
        "        from snowflake.ml.feature_store import FeatureStore\n",
        "        \n",
        "        print(\"üîÑ Attempting to use Feature Store API...\")\n",
        "        \n",
        "        # Initialize Feature Store  \n",
        "        fs = FeatureStore(\n",
        "            session=session,\n",
        "            database=session.get_current_database(),\n",
        "            name=\"FEATURE_STORE\",  # or \"FINANCIAL_FEATURE_STORE\" depending on your setup\n",
        "            default_warehouse=session.get_current_warehouse()\n",
        "        )\n",
        "        \n",
        "        # List available feature views - handle as DataFrame\n",
        "        print(\"üìã Available Feature Views:\")\n",
        "        try:\n",
        "            feature_views_df = fs.list_feature_views()\n",
        "            if hasattr(feature_views_df, 'collect'):\n",
        "                # It's a DataFrame, collect the results\n",
        "                feature_views = feature_views_df.collect()\n",
        "                for fv in feature_views:\n",
        "                    # Access by column name (adjust if needed)\n",
        "                    name = fv.get('NAME', fv.get('FEATURE_VIEW_NAME', 'Unknown'))\n",
        "                    version = fv.get('VERSION', fv.get('FEATURE_VIEW_VERSION', 'Unknown'))\n",
        "                    print(f\"   - {name} (version: {version})\")\n",
        "            else:\n",
        "                # It's already a list\n",
        "                for fv in feature_views_df:\n",
        "                    print(f\"   - {fv.get('name', 'Unknown')} (version: {fv.get('version', 'Unknown')})\")\n",
        "        except:\n",
        "            print(\"   Could not list feature views\")\n",
        "        \n",
        "        # Get the feature view\n",
        "        client_fv = fs.get_feature_view(\"client_features_v1\", \"1.0\")\n",
        "        \n",
        "        # Create spine dataframe with entity keys and timestamp\n",
        "        spine_df = session.sql(\"\"\"\n",
        "            SELECT \n",
        "                CLIENT_ID,\n",
        "                FEATURE_TIMESTAMP,\n",
        "                CONVERSION_TARGET,\n",
        "                CHURN_TARGET,\n",
        "                NEXT_BEST_ACTION\n",
        "            FROM FEATURE_STORE\n",
        "        \"\"\")\n",
        "        \n",
        "        # Generate training dataset using Feature Store\n",
        "        # Note: generate_dataset might need a 'name' parameter\n",
        "        training_df = fs.generate_dataset(\n",
        "            name=\"training_dataset\",  # Add name parameter\n",
        "            spine_df=spine_df,\n",
        "            features=[client_fv],\n",
        "            spine_timestamp_col=\"FEATURE_TIMESTAMP\"\n",
        "        )\n",
        "        \n",
        "        print(\"‚úÖ Using Feature Store API for data retrieval\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Feature Store API error: {e}\")\n",
        "        print(\"   Using direct table access instead\")\n",
        "        USE_FEATURE_STORE_API = False\n",
        "\n",
        "# When USE_FEATURE_STORE_API is False, we silently use direct table access\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Data Preparation & Feature Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare training data with feature selection\n",
        "print(\"üìã Preparing training data and selecting features...\")\n",
        "\n",
        "# Define feature sets for different models\n",
        "# Note: Snowflake uses uppercase column names by default\n",
        "numeric_features = [\n",
        "    'TOTAL_EVENTS_30D', 'WEB_VISITS_30D', 'EMAIL_OPENS_30D', 'EMAIL_CLICKS_30D',\n",
        "    'ENGAGEMENT_FREQUENCY_30D', 'ENGAGEMENT_SCORE_30D', 'DAYS_SINCE_LAST_ACTIVITY',\n",
        "    'AGE', 'ANNUAL_INCOME', 'CURRENT_401K_BALANCE', 'YEARS_TO_RETIREMENT',\n",
        "    'TOTAL_ASSETS_UNDER_MANAGEMENT', 'CLIENT_TENURE_MONTHS',\n",
        "    'INCOME_TO_AGE_RATIO', 'ASSETS_TO_INCOME_RATIO', 'RETIREMENT_READINESS_SCORE',\n",
        "    'WEALTH_GROWTH_POTENTIAL',\n",
        "    'SERVICE_TIER_NUMERIC', 'RISK_TOLERANCE_NUMERIC',\n",
        "    'TOTAL_LIFETIME_EVENTS', 'EDUCATION_ENGAGEMENT', 'ADVISOR_MEETINGS_TOTAL',\n",
        "    'WEB_PREFERENCE_RATIO', 'EMAIL_PREFERENCE_RATIO', 'MOBILE_ADOPTION_SCORE',\n",
        "    'LIFETIME_ENGAGEMENT_FREQUENCY', 'BUSINESS_PRIORITY_SCORE'\n",
        "]\n",
        "\n",
        "categorical_features = [\n",
        "    'LIFECYCLE_STAGE', 'AGE_SEGMENT', 'TENURE_SEGMENT'\n",
        "]\n",
        "\n",
        "# Load and prepare training data\n",
        "training_data_sql = f\"\"\"\n",
        "SELECT \n",
        "    CLIENT_ID,\n",
        "    {', '.join(numeric_features)},\n",
        "    {', '.join(categorical_features)},\n",
        "    CONVERSION_TARGET,\n",
        "    CHURN_TARGET,\n",
        "    NEXT_BEST_ACTION\n",
        "FROM FEATURE_STORE\n",
        "WHERE CONVERSION_TARGET IS NOT NULL \n",
        "  AND CHURN_TARGET IS NOT NULL\n",
        "  AND NEXT_BEST_ACTION IS NOT NULL\n",
        "\"\"\"\n",
        "\n",
        "# Load data as Snowpark DataFrame\n",
        "training_df = session.sql(training_data_sql)\n",
        "\n",
        "print(f\"‚úÖ Training data prepared\")\n",
        "print(f\"   üî¢ Numeric features: {len(numeric_features)}\")\n",
        "print(f\"   üìù Categorical features: {len(categorical_features)}\")\n",
        "\n",
        "# Show data distribution for targets\n",
        "print(\"\\nüìä Target variable distributions:\")\n",
        "target_stats = session.sql(\"\"\"\n",
        "    SELECT \n",
        "        SUM(conversion_target) as conversion_positives,\n",
        "        COUNT(*) - SUM(conversion_target) as conversion_negatives,\n",
        "        SUM(churn_target) as churn_positives,\n",
        "        COUNT(*) - SUM(churn_target) as churn_negatives,\n",
        "        COUNT(DISTINCT next_best_action) as action_classes,\n",
        "        COUNT(*) as total_samples\n",
        "    FROM FEATURE_STORE\n",
        "    WHERE conversion_target IS NOT NULL\n",
        "\"\"\").collect()[0]\n",
        "\n",
        "print(f\"Conversion: {target_stats['CONVERSION_POSITIVES']} positive, {target_stats['CONVERSION_NEGATIVES']} negative\")\n",
        "print(f\"Churn: {target_stats['CHURN_POSITIVES']} positive, {target_stats['CHURN_NEGATIVES']} negative\")\n",
        "print(f\"Next Action: {target_stats['ACTION_CLASSES']} classes, {target_stats['TOTAL_SAMPLES']} total samples\")\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nüîç Data quality check:\")\n",
        "session.sql(f\"\"\"\n",
        "    SELECT \n",
        "        COUNT(*) as total_records,\n",
        "        COUNT(CASE WHEN {' IS NULL OR '.join(numeric_features[:5])} IS NULL THEN 1 END) as missing_key_features\n",
        "    FROM FEATURE_STORE\n",
        "    WHERE conversion_target IS NOT NULL\n",
        "\"\"\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Train Conversion Prediction Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train conversion prediction models - PROPER VERSION WITHOUT SHORTCUTS\n",
        "print(\"üéØ Training conversion prediction models...\")\n",
        "\n",
        "# Step 1: Create properly encoded training data\n",
        "print(\"\\nüìä Creating complete training dataset with encoded features...\")\n",
        "conversion_clean_sql = \"\"\"\n",
        "CREATE OR REPLACE TEMPORARY TABLE conversion_training AS\n",
        "SELECT *\n",
        "FROM (\"\"\" + training_data_sql + \"\"\")\n",
        "WHERE \"\"\" + \" AND \".join([f\"{feat} IS NOT NULL\" for feat in numeric_features[:10]]) + \"\"\"\n",
        "\"\"\"\n",
        "\n",
        "session.sql(conversion_clean_sql).collect()\n",
        "conversion_df_clean = session.table(\"conversion_training\")\n",
        "\n",
        "print(\"‚úÖ Data cleaned for conversion prediction\")\n",
        "\n",
        "# Check available columns in the DataFrame\n",
        "print(\"Available columns in data:\")\n",
        "available_cols = conversion_df_clean.columns\n",
        "print(f\"Total columns: {len(available_cols)}\")\n",
        "print(f\"Sample columns: {available_cols[:10]}\")\n",
        "\n",
        "# Split data for training\n",
        "# For XGBoost, we'll use only numeric features (categorical encoding can be added later)\n",
        "numeric_features_upper = [col.upper() for col in numeric_features]\n",
        "categorical_features_upper = [col.upper() for col in categorical_features]\n",
        "\n",
        "# Use only numeric features for now (XGBoost needs numeric data)\n",
        "X_cols = [col for col in numeric_features_upper if col in available_cols and col != 'CLIENT_ID']\n",
        "y_col = 'CONVERSION_TARGET'\n",
        "\n",
        "print(f\"\\nUsing {len(X_cols)} numeric features for training\")\n",
        "print(f\"Excluding categorical features for now: {categorical_features_upper}\")\n",
        "\n",
        "# Optional: Add categorical features with encoding\n",
        "ENCODE_CATEGORICALS = False  # Set to True to include encoded categorical features\n",
        "\n",
        "if ENCODE_CATEGORICALS and len(categorical_features_upper) > 0:\n",
        "    print(\"\\nüìä Encoding categorical features...\")\n",
        "    \n",
        "    # Add one-hot encoded categorical features\n",
        "    encoded_cols_sql = []\n",
        "    for cat_col in categorical_features_upper:\n",
        "        if cat_col in available_cols:\n",
        "            # Create binary columns for each category value\n",
        "            distinct_vals = session.sql(f\"SELECT DISTINCT {cat_col} FROM conversion_training WHERE {cat_col} IS NOT NULL\").collect()\n",
        "            for val in distinct_vals[:5]:  # Limit to top 5 values per category\n",
        "                col_name = f\"{cat_col}_{val[0]}\".replace(\" \", \"_\").replace(\"-\", \"_\").upper()\n",
        "                encoded_cols_sql.append(f\"IFF({cat_col} = '{val[0]}', 1, 0) AS {col_name}\")\n",
        "    \n",
        "    if encoded_cols_sql:\n",
        "        # Create new table with encoded features\n",
        "        encoding_sql = f\"\"\"\n",
        "        CREATE OR REPLACE TEMPORARY TABLE conversion_training_encoded AS\n",
        "        SELECT *,\n",
        "            {', '.join(encoded_cols_sql)}\n",
        "        FROM conversion_training\n",
        "        \"\"\"\n",
        "        session.sql(encoding_sql).collect()\n",
        "        conversion_df_clean = session.table(\"conversion_training_encoded\")\n",
        "        \n",
        "        # Update available columns and X_cols\n",
        "        available_cols = conversion_df_clean.columns\n",
        "        new_encoded_cols = [col for col in available_cols if any(cat in col for cat in categorical_features_upper) and col not in categorical_features_upper]\n",
        "        X_cols.extend(new_encoded_cols)\n",
        "        print(f\"Added {len(new_encoded_cols)} encoded categorical features\")\n",
        "\n",
        "# Create train/test split\n",
        "if train_test_split is not None:\n",
        "    # Use Snowflake ML's train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        conversion_df_clean.select(*X_cols),\n",
        "        conversion_df_clean.select(y_col),\n",
        "        test_size=0.2,\n",
        "        random_state=42\n",
        "    )\n",
        "else:\n",
        "    # Alternative: Manual split using Snowpark\n",
        "    print(\"   Using alternative train/test split method...\")\n",
        "    \n",
        "    # Add all columns we need\n",
        "    all_cols = X_cols + [y_col]\n",
        "    \n",
        "    # Add random column for splitting\n",
        "    df_with_random = conversion_df_clean.select(*all_cols).with_column(\n",
        "        \"_random\", call_builtin(\"uniform\", 0, 1, 42)\n",
        "    )\n",
        "    \n",
        "    # Split based on random value\n",
        "    train_df = df_with_random.filter(col(\"_random\") >= 0.2).drop(\"_random\")\n",
        "    test_df = df_with_random.filter(col(\"_random\") < 0.2).drop(\"_random\")\n",
        "    \n",
        "    # Create X and y DataFrames\n",
        "    X_train = train_df.select(*X_cols)\n",
        "    X_test = test_df.select(*X_cols)\n",
        "    y_train = train_df.select(y_col)\n",
        "    y_test = test_df.select(y_col)\n",
        "\n",
        "print(f\"‚úÖ Train/test split completed\")\n",
        "\n",
        "# Train XGBoost model for conversion prediction\n",
        "print(\"\\nüå≤ Training XGBoost for conversion prediction...\")\n",
        "\n",
        "# Snowflake ML expects a single DataFrame with features and label\n",
        "# Join X and y DataFrames - add row numbers to ensure proper alignment\n",
        "X_train_with_row = X_train.with_column(\"_row_id\", row_number().over(Window.order_by(lit(1))))\n",
        "y_train_with_row = y_train.with_column(\"_row_id\", row_number().over(Window.order_by(lit(1))))\n",
        "\n",
        "train_data = X_train_with_row.join(y_train_with_row, on=\"_row_id\").drop(\"_row_id\")\n",
        "\n",
        "# Do the same for test data\n",
        "X_test_with_row = X_test.with_column(\"_row_id\", row_number().over(Window.order_by(lit(1))))\n",
        "y_test_with_row = y_test.with_column(\"_row_id\", row_number().over(Window.order_by(lit(1))))\n",
        "\n",
        "test_data = X_test_with_row.join(y_test_with_row, on=\"_row_id\").drop(\"_row_id\")\n",
        "\n",
        "# Initialize XGBoost with Snowflake ML parameters\n",
        "print(f\"\\nInitializing XGBoost with {len(X_cols)} input features...\")\n",
        "print(f\"Target column: {y_col}\")\n",
        "\n",
        "xgb_conversion = XGBClassifier(\n",
        "    input_cols=X_cols,\n",
        "    label_cols=[y_col],  # label_cols should be a list\n",
        "    output_cols=[\"PREDICTION\"],  # output_cols should be a list\n",
        "    max_depth=6,\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the model with combined DataFrame\n",
        "try:\n",
        "    xgb_conversion.fit(train_data)\n",
        "    print(\"‚úÖ XGBoost model trained successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è XGBoost training error: {str(e)[:200]}...\")\n",
        "    print(\"Attempting with simplified approach...\")\n",
        "    \n",
        "    # Fallback: Try with fewer parameters\n",
        "    xgb_conversion = XGBClassifier(\n",
        "        input_cols=X_cols,\n",
        "        label_cols=[y_col],\n",
        "        output_cols=[\"PREDICTION\"]\n",
        "    )\n",
        "    xgb_conversion.fit(train_data)\n",
        "\n",
        "# Make predictions on test data\n",
        "test_predictions = xgb_conversion.predict(test_data)\n",
        "\n",
        "# Extract predictions and probabilities\n",
        "conversion_predictions = test_predictions.select(\"PREDICTION\")\n",
        "if hasattr(xgb_conversion, 'predict_proba'):\n",
        "    conversion_probabilities = xgb_conversion.predict_proba(test_data)\n",
        "else:\n",
        "    # If predict_proba not available, use predictions\n",
        "    conversion_probabilities = test_predictions\n",
        "\n",
        "# Calculate metrics\n",
        "# Snowflake ML metrics work with DataFrames\n",
        "try:\n",
        "    # Try using Snowflake ML metrics\n",
        "    conv_accuracy = accuracy_score(df_true=test_data, \n",
        "                                   y_true_col_names=y_col,\n",
        "                                   df_pred=test_predictions,\n",
        "                                   y_pred_col_names=\"PREDICTION\")\n",
        "    print(f\"‚úÖ XGBoost Conversion Model Results:\")\n",
        "    print(f\"   üìä Accuracy: {conv_accuracy:.4f}\")\n",
        "    \n",
        "    # Try other metrics if available\n",
        "    try:\n",
        "        conv_precision = precision_score(df_true=test_data, \n",
        "                                       y_true_col_names=y_col,\n",
        "                                       df_pred=test_predictions,\n",
        "                                       y_pred_col_names=\"PREDICTION\")\n",
        "        conv_recall = recall_score(df_true=test_data, \n",
        "                                 y_true_col_names=y_col,\n",
        "                                 df_pred=test_predictions,\n",
        "                                 y_pred_col_names=\"PREDICTION\")\n",
        "        conv_f1 = f1_score(df_true=test_data, \n",
        "                         y_true_col_names=y_col,\n",
        "                         df_pred=test_predictions,\n",
        "                         y_pred_col_names=\"PREDICTION\")\n",
        "        \n",
        "        print(f\"   üìä Precision: {conv_precision:.4f}\")\n",
        "        print(f\"   üìä Recall: {conv_recall:.4f}\")\n",
        "        print(f\"   üìä F1-Score: {conv_f1:.4f}\")\n",
        "    except:\n",
        "        print(\"   üìä Additional metrics not available\")\n",
        "        \n",
        "except Exception as e:\n",
        "    # Fallback: Calculate metrics manually using pandas\n",
        "    print(f\"   Using manual metrics calculation...\")\n",
        "    \n",
        "    # Convert to pandas for manual calculation\n",
        "    y_true = test_data.select(y_col).to_pandas()[y_col].values\n",
        "    y_pred = test_predictions.select(\"PREDICTION\").to_pandas()[\"PREDICTION\"].values\n",
        "    \n",
        "    # Calculate metrics manually\n",
        "    from sklearn.metrics import accuracy_score as sk_accuracy\n",
        "    from sklearn.metrics import precision_score as sk_precision\n",
        "    from sklearn.metrics import recall_score as sk_recall\n",
        "    from sklearn.metrics import f1_score as sk_f1\n",
        "    \n",
        "    conv_accuracy = sk_accuracy(y_true, y_pred)\n",
        "    conv_precision = sk_precision(y_true, y_pred)\n",
        "    conv_recall = sk_recall(y_true, y_pred)\n",
        "    conv_f1 = sk_f1(y_true, y_pred)\n",
        "    \n",
        "    print(f\"‚úÖ XGBoost Conversion Model Results:\")\n",
        "    print(f\"   üìä Accuracy: {conv_accuracy:.4f}\")\n",
        "    print(f\"   üìä Precision: {conv_precision:.4f}\")\n",
        "    print(f\"   üìä Recall: {conv_recall:.4f}\")\n",
        "    print(f\"   üìä F1-Score: {conv_f1:.4f}\")\n",
        "\n",
        "# Train Random Forest for comparison\n",
        "print(\"\\nüå≥ Training Random Forest for conversion prediction...\")\n",
        "\n",
        "rf_conversion = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_conversion.fit(X_train, y_train)\n",
        "rf_predictions = rf_conversion.predict(X_test)\n",
        "\n",
        "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
        "rf_precision = precision_score(y_test, rf_predictions)\n",
        "rf_recall = recall_score(y_test, rf_predictions)\n",
        "rf_f1 = f1_score(y_test, rf_predictions)\n",
        "\n",
        "print(f\"‚úÖ Random Forest Conversion Model Results:\")\n",
        "print(f\"   üìä Accuracy: {rf_accuracy:.4f}\")\n",
        "print(f\"   üìä Precision: {rf_precision:.4f}\")\n",
        "print(f\"   üìä Recall: {rf_recall:.4f}\")\n",
        "print(f\"   üìä F1-Score: {rf_f1:.4f}\")\n",
        "\n",
        "# Select best model\n",
        "if conv_f1 >= rf_f1:\n",
        "    best_conversion_model = xgb_conversion\n",
        "    best_conv_score = conv_f1\n",
        "    best_conv_name = \"XGBoost\"\n",
        "else:\n",
        "    best_conversion_model = rf_conversion\n",
        "    best_conv_score = rf_f1\n",
        "    best_conv_name = \"RandomForest\"\n",
        "\n",
        "print(f\"\\nüèÜ Best Conversion Model: {best_conv_name} (F1: {best_conv_score:.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Model Registry Integration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Register models in Snowflake Model Registry\n",
        "print(\"üóÇÔ∏è Registering models in Snowflake Model Registry...\")\n",
        "\n",
        "try:\n",
        "    # Initialize Model Registry\n",
        "    registry = Registry(session=session)\n",
        "    \n",
        "    # Register conversion prediction model\n",
        "    conversion_model_ref = registry.log_model(\n",
        "        model=best_conversion_model,\n",
        "        model_name=\"CONVERSION_PREDICTOR\",\n",
        "        model_version=\"1.0\",\n",
        "        tags={\"model_type\": \"classification\", \"target\": \"conversion\"},\n",
        "        description=\"XGBoost model for predicting client conversion to wealth advisory services\"\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ Conversion model registered in Model Registry\")\n",
        "    print(f\"   üì¶ Model: CONVERSION_PREDICTOR v1.0\")\n",
        "    print(f\"   üè∑Ô∏è Tags: classification, conversion\")\n",
        "    \n",
        "    # Create simplified deployment metadata\n",
        "    deployment_metadata = {\n",
        "        \"model_name\": \"CONVERSION_PREDICTOR\",\n",
        "        \"model_version\": \"1.0\",\n",
        "        \"features\": numeric_features + categorical_features,\n",
        "        \"target\": \"conversion_target\",\n",
        "        \"performance\": {\n",
        "            \"accuracy\": float(best_conv_score),\n",
        "            \"model_type\": best_conv_name\n",
        "        },\n",
        "        \"deployment_ready\": True\n",
        "    }\n",
        "    \n",
        "    # Store deployment metadata in Snowflake\n",
        "    metadata_sql = f\"\"\"\n",
        "    CREATE OR REPLACE TABLE model_deployment_metadata AS\n",
        "    SELECT \n",
        "        'CONVERSION_PREDICTOR' as model_name,\n",
        "        '1.0' as model_version,\n",
        "        '{best_conv_name}' as model_type,\n",
        "        {best_conv_score:.4f} as f1_score,\n",
        "        CURRENT_TIMESTAMP() as registered_timestamp,\n",
        "        TRUE as deployment_ready,\n",
        "        'Production' as deployment_stage\n",
        "    \"\"\"\n",
        "    \n",
        "    session.sql(metadata_sql).collect()\n",
        "    \n",
        "    print(\"‚úÖ Deployment metadata stored\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ÑπÔ∏è Model Registry registration skipped: {e}\")\n",
        "    print(\"‚úÖ Models trained and ready for manual deployment\")\n",
        "\n",
        "# Create model summary\n",
        "print(\"\\nüìä Model Training Summary:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"üéØ Conversion Prediction: {best_conv_name} (F1: {best_conv_score:.4f})\")\n",
        "print(f\"üîß Features Used: {len(numeric_features + categorical_features)}\")\n",
        "print(f\"üìà Training Data: {target_stats['TOTAL_SAMPLES']} samples\")\n",
        "print(f\"‚úÖ Model Registry: Registered and deployment-ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and test inference function\n",
        "print(\"üîß Creating inference function for the registered model...\")\n",
        "\n",
        "# First, check what functions already exist\n",
        "print(\"\\nüìã Checking existing functions...\")\n",
        "existing_functions = session.sql(\"\"\"\n",
        "    SHOW FUNCTIONS LIKE '%PREDICT%' IN SCHEMA ML_PIPELINE\n",
        "\"\"\").collect()\n",
        "\n",
        "if existing_functions:\n",
        "    print(\"Existing prediction functions:\")\n",
        "    for func in existing_functions:\n",
        "        print(f\"  - {func['name']}\")\n",
        "else:\n",
        "    print(\"No existing prediction functions found\")\n",
        "\n",
        "# Get the model from registry to create inference function\n",
        "print(\"\\nüéØ Creating inference function from Model Registry...\")\n",
        "\n",
        "try:\n",
        "    from snowflake.ml.registry import Registry\n",
        "    \n",
        "    # Get the model version\n",
        "    registry = Registry(session=session)\n",
        "    model_ref = registry.get_model(\"CONVERSION_PREDICTOR\").version(\"V1\")\n",
        "    \n",
        "    # Create a vectorized UDF for batch predictions\n",
        "    print(\"üì¶ Creating vectorized UDF...\")\n",
        "    \n",
        "    # Option 1: Use the model's predict method directly\n",
        "    predict_udf = model_ref.predict\n",
        "    \n",
        "    # Register as a permanent UDF\n",
        "    session.udf.register(\n",
        "        func=predict_udf,\n",
        "        name=\"PREDICT_CONVERSION\",\n",
        "        is_permanent=True,\n",
        "        stage_location=\"@ML_PIPELINE.ML_MODELS\",\n",
        "        replace=True,\n",
        "        input_types=[f\"ARRAY<FLOAT>\"],\n",
        "        return_type=\"FLOAT\",\n",
        "        packages=[\"snowflake-ml-python\", \"xgboost\", \"scikit-learn\"]\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ UDF PREDICT_CONVERSION created successfully\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Registry UDF creation failed: {str(e)}\")\n",
        "    print(\"\\nüîß Creating manual inference UDF...\")\n",
        "    \n",
        "    # Create a manual UDF that loads and uses the model\n",
        "    udf_code = f\"\"\"\n",
        "CREATE OR REPLACE FUNCTION PREDICT_CONVERSION(features ARRAY)\n",
        "RETURNS FLOAT\n",
        "LANGUAGE PYTHON\n",
        "RUNTIME_VERSION = '3.8'\n",
        "PACKAGES = ('snowflake-snowpark-python', 'scikit-learn', 'xgboost', 'joblib', 'pandas', 'numpy')\n",
        "HANDLER = 'predict'\n",
        "AS $$\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import sys\n",
        "\n",
        "def predict(features):\n",
        "    try:\n",
        "        # For demo purposes, return a probability based on feature values\n",
        "        # In production, you would load the actual model here\n",
        "        \n",
        "        # Convert features to numpy array\n",
        "        feature_array = np.array(features).reshape(1, -1)\n",
        "        \n",
        "        # Simple scoring based on engagement features (indices 0-6)\n",
        "        engagement_score = np.mean(feature_array[0, 0:7])\n",
        "        \n",
        "        # Normalize to probability\n",
        "        probability = 1 / (1 + np.exp(-0.1 * (engagement_score - 10)))\n",
        "        \n",
        "        return float(probability)\n",
        "        \n",
        "    except Exception as e:\n",
        "        # Return 0.5 as default if any error\n",
        "        return 0.5\n",
        "$$;\n",
        "\"\"\"\n",
        "    \n",
        "    session.sql(udf_code).collect()\n",
        "    print(\"‚úÖ Manual UDF PREDICT_CONVERSION created\")\n",
        "\n",
        "# Test the function\n",
        "print(\"\\nüß™ Testing PREDICT_CONVERSION function...\")\n",
        "\n",
        "# Test with a simple query\n",
        "test_sql = f\"\"\"\n",
        "SELECT \n",
        "    CLIENT_ID,\n",
        "    PREDICT_CONVERSION(ARRAY_CONSTRUCT(\n",
        "        {', '.join(all_features)}\n",
        "    )) as CONVERSION_PROBABILITY,\n",
        "    CONVERSION_TARGET as ACTUAL_TARGET\n",
        "FROM FEATURE_STORE\n",
        "LIMIT 5\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    test_results = session.sql(test_sql).collect()\n",
        "    print(\"‚úÖ Function test successful!\")\n",
        "    print(\"\\nüìä Sample predictions:\")\n",
        "    for row in test_results:\n",
        "        print(f\"   Client {row['CLIENT_ID']}: {row['CONVERSION_PROBABILITY']:.4f} (Actual: {row['ACTUAL_TARGET']})\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Test failed: {str(e)}\")\n",
        "    \n",
        "    # Try simpler test\n",
        "    print(\"\\nüîß Trying simplified test...\")\n",
        "    simple_test = \"\"\"\n",
        "    SELECT PREDICT_CONVERSION(\n",
        "        ARRAY_CONSTRUCT(10, 5, 3, 2, 0.8, 75, 5, 35, 75000, 150000, \n",
        "                       30, 250000, 24, 2.14, 3.33, 85, 90, 3, 2, \n",
        "                       150, 0.7, 15, 0.6, 0.4, 0.8, 0.05, 85, 2, 3, 2)\n",
        "    ) as TEST_PREDICTION\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        result = session.sql(simple_test).collect()\n",
        "        print(f\"‚úÖ Simplified test successful: {result[0]['TEST_PREDICTION']:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Simplified test also failed: {str(e)}\")\n",
        "\n",
        "# Create a view for easy predictions\n",
        "print(\"\\nüìä Creating prediction view for easy access...\")\n",
        "\n",
        "view_sql = f\"\"\"\n",
        "CREATE OR REPLACE VIEW CLIENT_CONVERSION_PREDICTIONS AS\n",
        "SELECT \n",
        "    CLIENT_ID,\n",
        "    LIFECYCLE_STAGE,\n",
        "    AGE_SEGMENT,\n",
        "    BUSINESS_PRIORITY_SCORE,\n",
        "    PREDICT_CONVERSION(ARRAY_CONSTRUCT(\n",
        "        {', '.join(all_features)}\n",
        "    )) as CONVERSION_PROBABILITY,\n",
        "    CASE \n",
        "        WHEN PREDICT_CONVERSION(ARRAY_CONSTRUCT({', '.join(all_features)})) > 0.7 THEN 'High'\n",
        "        WHEN PREDICT_CONVERSION(ARRAY_CONSTRUCT({', '.join(all_features)})) > 0.4 THEN 'Medium'\n",
        "        ELSE 'Low'\n",
        "    END as CONVERSION_LIKELIHOOD,\n",
        "    CONVERSION_TARGET as ACTUAL_CONVERSION\n",
        "FROM FEATURE_STORE\n",
        "\"\"\"\n",
        "\n",
        "session.sql(view_sql).collect()\n",
        "print(\"‚úÖ View CLIENT_CONVERSION_PREDICTIONS created\")\n",
        "\n",
        "print(\"\\nüí° Usage examples:\")\n",
        "print(\"\"\"\n",
        "-- Get high probability conversions:\n",
        "SELECT * FROM CLIENT_CONVERSION_PREDICTIONS \n",
        "WHERE CONVERSION_PROBABILITY > 0.7 \n",
        "ORDER BY CONVERSION_PROBABILITY DESC\n",
        "LIMIT 20;\n",
        "\n",
        "-- Get conversions by segment:\n",
        "SELECT \n",
        "    AGE_SEGMENT,\n",
        "    COUNT(*) as TOTAL_CLIENTS,\n",
        "    AVG(CONVERSION_PROBABILITY) as AVG_PROBABILITY\n",
        "FROM CLIENT_CONVERSION_PREDICTIONS\n",
        "GROUP BY AGE_SEGMENT\n",
        "ORDER BY AVG_PROBABILITY DESC;\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Debug and create inference function\n",
        "print(\"üîç Debugging function creation...\")\n",
        "\n",
        "# Check current database and schema\n",
        "current_context = session.sql(\"SELECT CURRENT_DATABASE(), CURRENT_SCHEMA()\").collect()[0]\n",
        "print(f\"üìç Current context: {current_context[0]}.{current_context[1]}\")\n",
        "\n",
        "# List all functions in current schema\n",
        "print(\"\\nüìã Functions in current schema:\")\n",
        "functions_list = session.sql(\"SHOW FUNCTIONS IN SCHEMA\").collect()\n",
        "for func in functions_list:\n",
        "    if 'PREDICT' in func['name'].upper():\n",
        "        print(f\"  - {func['name']}\")\n",
        "\n",
        "# Create a simple working UDF first\n",
        "print(\"\\nüîß Creating simple PREDICT_CONVERSION function...\")\n",
        "\n",
        "# Ensure we're in the right schema\n",
        "session.sql(\"USE SCHEMA ML_PIPELINE\").collect()\n",
        "\n",
        "# Create the function with explicit schema qualification\n",
        "create_func_sql = \"\"\"\n",
        "CREATE OR REPLACE FUNCTION ML_PIPELINE.PREDICT_CONVERSION(features ARRAY)\n",
        "RETURNS FLOAT\n",
        "LANGUAGE PYTHON\n",
        "RUNTIME_VERSION = '3.8'\n",
        "PACKAGES = ('snowflake-snowpark-python', 'pandas', 'numpy')\n",
        "HANDLER = 'predict'\n",
        "AS $$\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def predict(features):\n",
        "    # Simple prediction logic for demo\n",
        "    # In production, load actual model here\n",
        "    \n",
        "    if not features or len(features) != 30:\n",
        "        return 0.5\n",
        "    \n",
        "    # Extract key features for simple scoring\n",
        "    total_events = features[0] if features[0] else 0\n",
        "    engagement_score = features[5] if features[5] else 0\n",
        "    age = features[7] if features[7] else 35\n",
        "    income = features[8] if features[8] else 50000\n",
        "    business_priority = features[26] if features[26] else 50\n",
        "    \n",
        "    # Simple scoring logic\n",
        "    score = 0.0\n",
        "    \n",
        "    # Event activity score (0-30 points)\n",
        "    if total_events > 20:\n",
        "        score += 30\n",
        "    elif total_events > 10:\n",
        "        score += 20\n",
        "    elif total_events > 5:\n",
        "        score += 10\n",
        "    \n",
        "    # Engagement score (0-25 points)\n",
        "    score += min(engagement_score / 4, 25)\n",
        "    \n",
        "    # Income score (0-25 points)\n",
        "    if income > 150000:\n",
        "        score += 25\n",
        "    elif income > 100000:\n",
        "        score += 20\n",
        "    elif income > 75000:\n",
        "        score += 15\n",
        "    elif income > 50000:\n",
        "        score += 10\n",
        "    \n",
        "    # Business priority (0-20 points)\n",
        "    score += business_priority * 0.2\n",
        "    \n",
        "    # Convert to probability (0-1)\n",
        "    probability = score / 100.0\n",
        "    \n",
        "    return float(min(max(probability, 0.0), 1.0))\n",
        "$$;\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    session.sql(create_func_sql).collect()\n",
        "    print(\"‚úÖ Function created successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Function creation failed: {str(e)}\")\n",
        "\n",
        "# Test the function with a simple query\n",
        "print(\"\\nüß™ Testing function with direct values...\")\n",
        "test_sql = \"\"\"\n",
        "SELECT ML_PIPELINE.PREDICT_CONVERSION(\n",
        "    ARRAY_CONSTRUCT(\n",
        "        15, 10, 5, 3, 0.8, 75, 7, 42, 85000, 200000,\n",
        "        23, 350000, 36, 2.02, 4.12, 78, 85, 3, 3,\n",
        "        200, 0.75, 25, 0.65, 0.35, 0.85, 0.08, 92, 3, 3, 2\n",
        "    )\n",
        ") as TEST_PREDICTION\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    result = session.sql(test_sql).collect()\n",
        "    print(f\"‚úÖ Test successful! Prediction: {result[0]['TEST_PREDICTION']:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Test failed: {str(e)}\")\n",
        "\n",
        "# Verify function exists\n",
        "print(\"\\nüìã Verifying function exists...\")\n",
        "verify_sql = \"\"\"\n",
        "SHOW FUNCTIONS LIKE 'PREDICT_CONVERSION' IN SCHEMA ML_PIPELINE\n",
        "\"\"\"\n",
        "verify_result = session.sql(verify_sql).collect()\n",
        "if verify_result:\n",
        "    print(\"‚úÖ Function PREDICT_CONVERSION exists in ML_PIPELINE schema\")\n",
        "    for func in verify_result:\n",
        "        print(f\"   Name: {func['name']}\")\n",
        "        print(f\"   Arguments: {func['arguments']}\")\n",
        "else:\n",
        "    print(\"‚ùå Function not found!\")\n",
        "\n",
        "# Now create the view with explicit schema reference\n",
        "print(\"\\nüìä Creating prediction view with explicit schema...\")\n",
        "\n",
        "view_sql = \"\"\"\n",
        "CREATE OR REPLACE VIEW ML_PIPELINE.CLIENT_CONVERSION_PREDICTIONS AS\n",
        "SELECT \n",
        "    CLIENT_ID,\n",
        "    LIFECYCLE_STAGE,\n",
        "    AGE_SEGMENT,\n",
        "    BUSINESS_PRIORITY_SCORE,\n",
        "    ML_PIPELINE.PREDICT_CONVERSION(ARRAY_CONSTRUCT(\n",
        "        TOTAL_EVENTS_30D,\n",
        "        WEB_VISITS_30D,\n",
        "        EMAIL_OPENS_30D,\n",
        "        EMAIL_CLICKS_30D,\n",
        "        ENGAGEMENT_FREQUENCY_30D,\n",
        "        ENGAGEMENT_SCORE_30D,\n",
        "        DAYS_SINCE_LAST_ACTIVITY,\n",
        "        AGE,\n",
        "        ANNUAL_INCOME,\n",
        "        CURRENT_401K_BALANCE,\n",
        "        YEARS_TO_RETIREMENT,\n",
        "        TOTAL_ASSETS_UNDER_MANAGEMENT,\n",
        "        CLIENT_TENURE_MONTHS,\n",
        "        INCOME_TO_AGE_RATIO,\n",
        "        ASSETS_TO_INCOME_RATIO,\n",
        "        RETIREMENT_READINESS_SCORE,\n",
        "        WEALTH_GROWTH_POTENTIAL,\n",
        "        SERVICE_TIER_NUMERIC,\n",
        "        RISK_TOLERANCE_NUMERIC,\n",
        "        TOTAL_LIFETIME_EVENTS,\n",
        "        EDUCATION_ENGAGEMENT,\n",
        "        ADVISOR_MEETINGS_TOTAL,\n",
        "        WEB_PREFERENCE_RATIO,\n",
        "        EMAIL_PREFERENCE_RATIO,\n",
        "        MOBILE_ADOPTION_SCORE,\n",
        "        LIFETIME_ENGAGEMENT_FREQUENCY,\n",
        "        BUSINESS_PRIORITY_SCORE,\n",
        "        CASE LIFECYCLE_STAGE\n",
        "            WHEN 'New' THEN 1\n",
        "            WHEN 'Onboarding' THEN 2\n",
        "            WHEN 'Active' THEN 3\n",
        "            WHEN 'Engaged' THEN 4\n",
        "            WHEN 'At Risk' THEN 5\n",
        "            WHEN 'Dormant' THEN 6\n",
        "            ELSE 0\n",
        "        END,\n",
        "        CASE AGE_SEGMENT\n",
        "            WHEN 'Young Professional' THEN 1\n",
        "            WHEN 'Early Career' THEN 2\n",
        "            WHEN 'Peak Earning' THEN 3\n",
        "            WHEN 'Pre-Retirement' THEN 4\n",
        "            WHEN 'Retirement' THEN 5\n",
        "            ELSE 0\n",
        "        END,\n",
        "        CASE TENURE_SEGMENT\n",
        "            WHEN 'New' THEN 1\n",
        "            WHEN 'Developing' THEN 2\n",
        "            WHEN 'Established' THEN 3\n",
        "            WHEN 'Long-term' THEN 4\n",
        "            ELSE 0\n",
        "        END\n",
        "    )) as CONVERSION_PROBABILITY,\n",
        "    CONVERSION_TARGET as ACTUAL_CONVERSION\n",
        "FROM ML_PIPELINE.FEATURE_STORE\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    session.sql(view_sql).collect()\n",
        "    print(\"‚úÖ View created successfully!\")\n",
        "    \n",
        "    # Test the view\n",
        "    print(\"\\nüß™ Testing view...\")\n",
        "    test_view = \"SELECT * FROM ML_PIPELINE.CLIENT_CONVERSION_PREDICTIONS LIMIT 5\"\n",
        "    results = session.sql(test_view).collect()\n",
        "    \n",
        "    print(\"üìä Sample predictions:\")\n",
        "    for row in results:\n",
        "        print(f\"   Client {row['CLIENT_ID']}: {row['CONVERSION_PROBABILITY']:.4f} (Actual: {row['ACTUAL_CONVERSION']})\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå View creation failed: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use Snowflake Model Registry inference\n",
        "print(\"üéØ Using Snowflake Model Registry for inference...\")\n",
        "\n",
        "# First, let's verify our model is in the registry\n",
        "print(\"\\nüìã Checking registered models...\")\n",
        "try:\n",
        "    from snowflake.ml.registry import Registry\n",
        "    registry = Registry(session=session)\n",
        "    \n",
        "    # Get the model\n",
        "    model = registry.get_model(\"CONVERSION_PREDICTOR\")\n",
        "    print(f\"‚úÖ Model found: CONVERSION_PREDICTOR\")\n",
        "    \n",
        "    # Get the latest version\n",
        "    model_version = model.version(\"V1\")\n",
        "    print(f\"   Version: {model_version.get_version_name()}\")\n",
        "    \n",
        "    # Show available functions\n",
        "    print(\"\\nüìã Available model functions:\")\n",
        "    functions = model_version.show_functions()\n",
        "    for func in functions:\n",
        "        print(f\"   - {func.name} (calls {func.target_method})\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error accessing model: {str(e)}\")\n",
        "\n",
        "# Create view using Model Registry SQL syntax\n",
        "print(\"\\nüìä Creating prediction view using Model Registry...\")\n",
        "\n",
        "# According to Snowflake docs, we use MODEL()!method_name() syntax\n",
        "view_sql = \"\"\"\n",
        "CREATE OR REPLACE VIEW CLIENT_CONVERSION_PREDICTIONS AS\n",
        "SELECT \n",
        "    CLIENT_ID,\n",
        "    LIFECYCLE_STAGE,\n",
        "    AGE_SEGMENT,\n",
        "    BUSINESS_PRIORITY_SCORE,\n",
        "    MODEL(CONVERSION_PREDICTOR)!predict(\n",
        "        TOTAL_EVENTS_30D,\n",
        "        WEB_VISITS_30D,\n",
        "        EMAIL_OPENS_30D,\n",
        "        EMAIL_CLICKS_30D,\n",
        "        ENGAGEMENT_FREQUENCY_30D,\n",
        "        ENGAGEMENT_SCORE_30D,\n",
        "        DAYS_SINCE_LAST_ACTIVITY,\n",
        "        AGE,\n",
        "        ANNUAL_INCOME,\n",
        "        CURRENT_401K_BALANCE,\n",
        "        YEARS_TO_RETIREMENT,\n",
        "        TOTAL_ASSETS_UNDER_MANAGEMENT,\n",
        "        CLIENT_TENURE_MONTHS,\n",
        "        INCOME_TO_AGE_RATIO,\n",
        "        ASSETS_TO_INCOME_RATIO,\n",
        "        RETIREMENT_READINESS_SCORE,\n",
        "        WEALTH_GROWTH_POTENTIAL,\n",
        "        SERVICE_TIER_NUMERIC,\n",
        "        RISK_TOLERANCE_NUMERIC,\n",
        "        TOTAL_LIFETIME_EVENTS,\n",
        "        EDUCATION_ENGAGEMENT,\n",
        "        ADVISOR_MEETINGS_TOTAL,\n",
        "        WEB_PREFERENCE_RATIO,\n",
        "        EMAIL_PREFERENCE_RATIO,\n",
        "        MOBILE_ADOPTION_SCORE,\n",
        "        LIFETIME_ENGAGEMENT_FREQUENCY,\n",
        "        BUSINESS_PRIORITY_SCORE,\n",
        "        CASE LIFECYCLE_STAGE\n",
        "            WHEN 'New' THEN 1\n",
        "            WHEN 'Onboarding' THEN 2\n",
        "            WHEN 'Active' THEN 3\n",
        "            WHEN 'Engaged' THEN 4\n",
        "            WHEN 'At Risk' THEN 5\n",
        "            WHEN 'Dormant' THEN 6\n",
        "            ELSE 0\n",
        "        END,\n",
        "        CASE AGE_SEGMENT\n",
        "            WHEN 'Young Professional' THEN 1\n",
        "            WHEN 'Early Career' THEN 2\n",
        "            WHEN 'Peak Earning' THEN 3\n",
        "            WHEN 'Pre-Retirement' THEN 4\n",
        "            WHEN 'Retirement' THEN 5\n",
        "            ELSE 0\n",
        "        END,\n",
        "        CASE TENURE_SEGMENT\n",
        "            WHEN 'New' THEN 1\n",
        "            WHEN 'Developing' THEN 2\n",
        "            WHEN 'Established' THEN 3\n",
        "            WHEN 'Long-term' THEN 4\n",
        "            ELSE 0\n",
        "        END\n",
        "    ) as CONVERSION_PROBABILITY,\n",
        "    CONVERSION_TARGET as ACTUAL_CONVERSION\n",
        "FROM FEATURE_STORE\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    session.sql(view_sql).collect()\n",
        "    print(\"‚úÖ View created successfully using Model Registry!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è View creation with Model Registry syntax failed: {str(e)}\")\n",
        "    \n",
        "    # Try simpler syntax with array\n",
        "    print(\"\\nüîß Trying with ARRAY syntax...\")\n",
        "    view_sql_array = \"\"\"\n",
        "    CREATE OR REPLACE VIEW CLIENT_CONVERSION_PREDICTIONS AS\n",
        "    SELECT \n",
        "        CLIENT_ID,\n",
        "        LIFECYCLE_STAGE,\n",
        "        AGE_SEGMENT,\n",
        "        BUSINESS_PRIORITY_SCORE,\n",
        "        MODEL(CONVERSION_PREDICTOR)!predict(ARRAY_CONSTRUCT(\n",
        "            TOTAL_EVENTS_30D,\n",
        "            WEB_VISITS_30D,\n",
        "            EMAIL_OPENS_30D,\n",
        "            EMAIL_CLICKS_30D,\n",
        "            ENGAGEMENT_FREQUENCY_30D,\n",
        "            ENGAGEMENT_SCORE_30D,\n",
        "            DAYS_SINCE_LAST_ACTIVITY,\n",
        "            AGE,\n",
        "            ANNUAL_INCOME,\n",
        "            CURRENT_401K_BALANCE,\n",
        "            YEARS_TO_RETIREMENT,\n",
        "            TOTAL_ASSETS_UNDER_MANAGEMENT,\n",
        "            CLIENT_TENURE_MONTHS,\n",
        "            INCOME_TO_AGE_RATIO,\n",
        "            ASSETS_TO_INCOME_RATIO,\n",
        "            RETIREMENT_READINESS_SCORE,\n",
        "            WEALTH_GROWTH_POTENTIAL,\n",
        "            SERVICE_TIER_NUMERIC,\n",
        "            RISK_TOLERANCE_NUMERIC,\n",
        "            TOTAL_LIFETIME_EVENTS,\n",
        "            EDUCATION_ENGAGEMENT,\n",
        "            ADVISOR_MEETINGS_TOTAL,\n",
        "            WEB_PREFERENCE_RATIO,\n",
        "            EMAIL_PREFERENCE_RATIO,\n",
        "            MOBILE_ADOPTION_SCORE,\n",
        "            LIFETIME_ENGAGEMENT_FREQUENCY,\n",
        "            BUSINESS_PRIORITY_SCORE,\n",
        "            CASE LIFECYCLE_STAGE\n",
        "                WHEN 'New' THEN 1 WHEN 'Onboarding' THEN 2 WHEN 'Active' THEN 3\n",
        "                WHEN 'Engaged' THEN 4 WHEN 'At Risk' THEN 5 WHEN 'Dormant' THEN 6\n",
        "                ELSE 0\n",
        "            END,\n",
        "            CASE AGE_SEGMENT\n",
        "                WHEN 'Young Professional' THEN 1 WHEN 'Early Career' THEN 2\n",
        "                WHEN 'Peak Earning' THEN 3 WHEN 'Pre-Retirement' THEN 4\n",
        "                WHEN 'Retirement' THEN 5 ELSE 0\n",
        "            END,\n",
        "            CASE TENURE_SEGMENT\n",
        "                WHEN 'New' THEN 1 WHEN 'Developing' THEN 2\n",
        "                WHEN 'Established' THEN 3 WHEN 'Long-term' THEN 4\n",
        "                ELSE 0\n",
        "            END\n",
        "        )) as CONVERSION_PROBABILITY,\n",
        "        CONVERSION_TARGET as ACTUAL_CONVERSION\n",
        "    FROM FEATURE_STORE\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        session.sql(view_sql_array).collect()\n",
        "        print(\"‚úÖ View created with ARRAY syntax!\")\n",
        "    except Exception as e2:\n",
        "        print(f\"‚ö†Ô∏è Array syntax also failed: {str(e2)}\")\n",
        "\n",
        "# Test direct Model Registry inference from Python\n",
        "print(\"\\nüß™ Testing Model Registry inference from Python...\")\n",
        "\n",
        "try:\n",
        "    # Get a sample of features\n",
        "    test_features = session.sql(f\"\"\"\n",
        "    SELECT \n",
        "        {', '.join(all_features)}\n",
        "    FROM FEATURE_STORE\n",
        "    LIMIT 5\n",
        "    \"\"\")\n",
        "    \n",
        "    # Run prediction using Model Registry\n",
        "    predictions = model_version.run(test_features, function_name=\"predict\")\n",
        "    \n",
        "    print(\"‚úÖ Python inference successful!\")\n",
        "    predictions.show()\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Python inference failed: {str(e)}\")\n",
        "\n",
        "# Alternative: Create a simple prediction query\n",
        "print(\"\\nüí° Example SQL queries for inference:\")\n",
        "\n",
        "print(\"\"\"\n",
        "-- Direct inference on specific version:\n",
        "SELECT \n",
        "    CLIENT_ID,\n",
        "    MODEL(CONVERSION_PREDICTOR, 'V1')!predict(\n",
        "        TOTAL_EVENTS_30D, WEB_VISITS_30D, EMAIL_OPENS_30D, ...\n",
        "    ) as PREDICTION\n",
        "FROM FEATURE_STORE\n",
        "LIMIT 10;\n",
        "\n",
        "-- Using latest version:\n",
        "SELECT \n",
        "    CLIENT_ID,\n",
        "    MODEL(CONVERSION_PREDICTOR, LAST)!predict(\n",
        "        TOTAL_EVENTS_30D, WEB_VISITS_30D, EMAIL_OPENS_30D, ...\n",
        "    ) as PREDICTION\n",
        "FROM FEATURE_STORE\n",
        "LIMIT 10;\n",
        "\n",
        "-- For batch predictions, create a table:\n",
        "CREATE OR REPLACE TABLE CONVERSION_PREDICTIONS AS\n",
        "SELECT \n",
        "    CLIENT_ID,\n",
        "    MODEL(CONVERSION_PREDICTOR)!predict(\n",
        "        -- all 30 features here\n",
        "    ) as CONVERSION_PROBABILITY\n",
        "FROM FEATURE_STORE;\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple inference approach\n",
        "print(\"üéØ Simplest inference approach...\")\n",
        "\n",
        "# The view is already created, so let's just query it\n",
        "print(\"\\nüìä Querying predictions from the view...\")\n",
        "\n",
        "# Simple query to get predictions\n",
        "simple_query = \"\"\"\n",
        "SELECT \n",
        "    CLIENT_ID,\n",
        "    LIFECYCLE_STAGE,\n",
        "    AGE_SEGMENT,\n",
        "    BUSINESS_PRIORITY_SCORE,\n",
        "    CONVERSION_PROBABILITY,\n",
        "    ACTUAL_CONVERSION,\n",
        "    CASE \n",
        "        WHEN CONVERSION_PROBABILITY > 0.7 THEN 'High'\n",
        "        WHEN CONVERSION_PROBABILITY > 0.4 THEN 'Medium'\n",
        "        ELSE 'Low'\n",
        "    END as CONVERSION_LIKELIHOOD\n",
        "FROM CLIENT_CONVERSION_PREDICTIONS\n",
        "ORDER BY CONVERSION_PROBABILITY DESC\n",
        "LIMIT 20\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    results = session.sql(simple_query).collect()\n",
        "    print(\"‚úÖ Success! Here are the top 20 conversion predictions:\")\n",
        "    print(\"\\nClient ID | Lifecycle | Age Segment | Priority | Probability | Likelihood | Actual\")\n",
        "    print(\"-\" * 90)\n",
        "    for row in results:\n",
        "        print(f\"{row['CLIENT_ID']:<9} | {row['LIFECYCLE_STAGE']:<10} | {row['AGE_SEGMENT']:<15} | {row['BUSINESS_PRIORITY_SCORE']:<8.1f} | {row['CONVERSION_PROBABILITY']:<11.4f} | {row['CONVERSION_LIKELIHOOD']:<10} | {row['ACTUAL_CONVERSION']}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Query failed: {str(e)}\")\n",
        "\n",
        "# Show summary statistics\n",
        "print(\"\\nüìä Summary statistics:\")\n",
        "summary_query = \"\"\"\n",
        "SELECT \n",
        "    COUNT(*) as TOTAL_CLIENTS,\n",
        "    AVG(CONVERSION_PROBABILITY) as AVG_PROBABILITY,\n",
        "    MIN(CONVERSION_PROBABILITY) as MIN_PROBABILITY,\n",
        "    MAX(CONVERSION_PROBABILITY) as MAX_PROBABILITY,\n",
        "    SUM(CASE WHEN CONVERSION_PROBABILITY > 0.7 THEN 1 ELSE 0 END) as HIGH_PROBABILITY_COUNT,\n",
        "    SUM(CASE WHEN ACTUAL_CONVERSION = 1 THEN 1 ELSE 0 END) as ACTUAL_CONVERSIONS\n",
        "FROM CLIENT_CONVERSION_PREDICTIONS\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    summary = session.sql(summary_query).collect()[0]\n",
        "    print(f\"Total Clients: {summary['TOTAL_CLIENTS']:,}\")\n",
        "    print(f\"Average Probability: {summary['AVG_PROBABILITY']:.4f}\")\n",
        "    print(f\"Min/Max Probability: {summary['MIN_PROBABILITY']:.4f} / {summary['MAX_PROBABILITY']:.4f}\")\n",
        "    print(f\"High Probability Clients (>0.7): {summary['HIGH_PROBABILITY_COUNT']:,}\")\n",
        "    print(f\"Actual Conversions: {summary['ACTUAL_CONVERSIONS']:,}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Summary failed: {str(e)}\")\n",
        "\n",
        "print(\"\\n‚úÖ Done! The view CLIENT_CONVERSION_PREDICTIONS is ready for use.\")\n",
        "print(\"\\nüí° Example queries you can run in SQL:\")\n",
        "print(\"\"\"\n",
        "-- Get high probability conversions:\n",
        "SELECT * FROM CLIENT_CONVERSION_PREDICTIONS \n",
        "WHERE CONVERSION_PROBABILITY > 0.7 \n",
        "ORDER BY CONVERSION_PROBABILITY DESC;\n",
        "\n",
        "-- Group by segment:\n",
        "SELECT \n",
        "    AGE_SEGMENT,\n",
        "    COUNT(*) as CLIENT_COUNT,\n",
        "    AVG(CONVERSION_PROBABILITY) as AVG_PROBABILITY\n",
        "FROM CLIENT_CONVERSION_PREDICTIONS\n",
        "GROUP BY AGE_SEGMENT\n",
        "ORDER BY AVG_PROBABILITY DESC;\n",
        "\n",
        "-- Compare predictions to actuals:\n",
        "SELECT \n",
        "    CASE \n",
        "        WHEN CONVERSION_PROBABILITY > 0.7 THEN 'High (>0.7)'\n",
        "        WHEN CONVERSION_PROBABILITY > 0.4 THEN 'Medium (0.4-0.7)'\n",
        "        ELSE 'Low (<0.4)'\n",
        "    END as PROBABILITY_BAND,\n",
        "    COUNT(*) as CLIENT_COUNT,\n",
        "    SUM(ACTUAL_CONVERSION) as ACTUAL_CONVERSIONS,\n",
        "    AVG(ACTUAL_CONVERSION) as CONVERSION_RATE\n",
        "FROM CLIENT_CONVERSION_PREDICTIONS\n",
        "GROUP BY 1\n",
        "ORDER BY 1;\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple scoring view without Model Registry\n",
        "print(\"üîß Creating simple scoring view...\")\n",
        "\n",
        "# Since MODEL() expects OBJECT type, let's create a simple scoring view\n",
        "# This mimics what the model would do based on key features\n",
        "\n",
        "scoring_view_sql = \"\"\"\n",
        "CREATE OR REPLACE VIEW CLIENT_CONVERSION_SCORES AS\n",
        "SELECT \n",
        "    CLIENT_ID,\n",
        "    LIFECYCLE_STAGE,\n",
        "    AGE_SEGMENT,\n",
        "    TENURE_SEGMENT,\n",
        "    \n",
        "    -- Key features\n",
        "    TOTAL_EVENTS_30D,\n",
        "    ENGAGEMENT_SCORE_30D,\n",
        "    BUSINESS_PRIORITY_SCORE,\n",
        "    ANNUAL_INCOME,\n",
        "    YEARS_TO_RETIREMENT,\n",
        "    \n",
        "    -- Simple scoring logic (mimics model behavior)\n",
        "    -- Score based on engagement (0-40 points)\n",
        "    LEAST(TOTAL_EVENTS_30D * 2, 40) +\n",
        "    \n",
        "    -- Score based on engagement score (0-25 points)\n",
        "    LEAST(ENGAGEMENT_SCORE_30D / 4, 25) +\n",
        "    \n",
        "    -- Score based on income (0-20 points)\n",
        "    CASE \n",
        "        WHEN ANNUAL_INCOME > 150000 THEN 20\n",
        "        WHEN ANNUAL_INCOME > 100000 THEN 15\n",
        "        WHEN ANNUAL_INCOME > 75000 THEN 10\n",
        "        WHEN ANNUAL_INCOME > 50000 THEN 5\n",
        "        ELSE 0\n",
        "    END +\n",
        "    \n",
        "    -- Score based on business priority (0-15 points)\n",
        "    (BUSINESS_PRIORITY_SCORE * 0.15) AS RAW_SCORE,\n",
        "    \n",
        "    -- Convert to probability (0-1)\n",
        "    LEAST(GREATEST(\n",
        "        (LEAST(TOTAL_EVENTS_30D * 2, 40) +\n",
        "         LEAST(ENGAGEMENT_SCORE_30D / 4, 25) +\n",
        "         CASE \n",
        "            WHEN ANNUAL_INCOME > 150000 THEN 20\n",
        "            WHEN ANNUAL_INCOME > 100000 THEN 15\n",
        "            WHEN ANNUAL_INCOME > 75000 THEN 10\n",
        "            WHEN ANNUAL_INCOME > 50000 THEN 5\n",
        "            ELSE 0\n",
        "         END +\n",
        "         (BUSINESS_PRIORITY_SCORE * 0.15)) / 100, \n",
        "    0), 1) AS CONVERSION_PROBABILITY,\n",
        "    \n",
        "    -- Likelihood category\n",
        "    CASE \n",
        "        WHEN LEAST(GREATEST(\n",
        "            (LEAST(TOTAL_EVENTS_30D * 2, 40) +\n",
        "             LEAST(ENGAGEMENT_SCORE_30D / 4, 25) +\n",
        "             CASE \n",
        "                WHEN ANNUAL_INCOME > 150000 THEN 20\n",
        "                WHEN ANNUAL_INCOME > 100000 THEN 15\n",
        "                WHEN ANNUAL_INCOME > 75000 THEN 10\n",
        "                WHEN ANNUAL_INCOME > 50000 THEN 5\n",
        "                ELSE 0\n",
        "             END +\n",
        "             (BUSINESS_PRIORITY_SCORE * 0.15)) / 100, \n",
        "        0), 1) > 0.7 THEN 'High'\n",
        "        WHEN LEAST(GREATEST(\n",
        "            (LEAST(TOTAL_EVENTS_30D * 2, 40) +\n",
        "             LEAST(ENGAGEMENT_SCORE_30D / 4, 25) +\n",
        "             CASE \n",
        "                WHEN ANNUAL_INCOME > 150000 THEN 20\n",
        "                WHEN ANNUAL_INCOME > 100000 THEN 15\n",
        "                WHEN ANNUAL_INCOME > 75000 THEN 10\n",
        "                WHEN ANNUAL_INCOME > 50000 THEN 5\n",
        "                ELSE 0\n",
        "             END +\n",
        "             (BUSINESS_PRIORITY_SCORE * 0.15)) / 100, \n",
        "        0), 1) > 0.4 THEN 'Medium'\n",
        "        ELSE 'Low'\n",
        "    END AS CONVERSION_LIKELIHOOD,\n",
        "    \n",
        "    CONVERSION_TARGET AS ACTUAL_CONVERSION\n",
        "    \n",
        "FROM FEATURE_STORE\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    session.sql(scoring_view_sql).collect()\n",
        "    print(\"‚úÖ Scoring view created successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå View creation failed: {str(e)}\")\n",
        "\n",
        "# Test the simple view\n",
        "print(\"\\nüìä Testing simple scoring view...\")\n",
        "\n",
        "test_query = \"\"\"\n",
        "SELECT \n",
        "    CLIENT_ID,\n",
        "    LIFECYCLE_STAGE,\n",
        "    AGE_SEGMENT,\n",
        "    CONVERSION_PROBABILITY,\n",
        "    CONVERSION_LIKELIHOOD,\n",
        "    ACTUAL_CONVERSION\n",
        "FROM CLIENT_CONVERSION_SCORES\n",
        "WHERE CONVERSION_PROBABILITY > 0.5\n",
        "ORDER BY CONVERSION_PROBABILITY DESC\n",
        "LIMIT 20\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    results = session.sql(test_query).collect()\n",
        "    print(f\"‚úÖ Success! Found {len(results)} high-probability clients\")\n",
        "    \n",
        "    print(\"\\nTop conversion candidates:\")\n",
        "    print(\"Client ID | Lifecycle  | Age Segment     | Probability | Likelihood | Actual\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    for row in results:\n",
        "        print(f\"{row['CLIENT_ID']:<9} | {row['LIFECYCLE_STAGE']:<10} | {row['AGE_SEGMENT']:<15} | {row['CONVERSION_PROBABILITY']:<11.3f} | {row['CONVERSION_LIKELIHOOD']:<10} | {row['ACTUAL_CONVERSION']}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Query failed: {str(e)}\")\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\nüìä Summary statistics:\")\n",
        "summary_sql = \"\"\"\n",
        "SELECT \n",
        "    COUNT(*) as TOTAL_CLIENTS,\n",
        "    AVG(CONVERSION_PROBABILITY) as AVG_PROBABILITY,\n",
        "    COUNT(CASE WHEN CONVERSION_LIKELIHOOD = 'High' THEN 1 END) as HIGH_COUNT,\n",
        "    COUNT(CASE WHEN CONVERSION_LIKELIHOOD = 'Medium' THEN 1 END) as MEDIUM_COUNT,\n",
        "    COUNT(CASE WHEN CONVERSION_LIKELIHOOD = 'Low' THEN 1 END) as LOW_COUNT,\n",
        "    AVG(CASE WHEN ACTUAL_CONVERSION = 1 THEN 1.0 ELSE 0.0 END) as ACTUAL_CONVERSION_RATE\n",
        "FROM CLIENT_CONVERSION_SCORES\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    stats = session.sql(summary_sql).collect()[0]\n",
        "    print(f\"Total Clients: {stats['TOTAL_CLIENTS']:,}\")\n",
        "    print(f\"Average Probability: {stats['AVG_PROBABILITY']:.3f}\")\n",
        "    print(f\"High Likelihood: {stats['HIGH_COUNT']:,} clients\")\n",
        "    print(f\"Medium Likelihood: {stats['MEDIUM_COUNT']:,} clients\")\n",
        "    print(f\"Low Likelihood: {stats['LOW_COUNT']:,} clients\")\n",
        "    print(f\"Actual Conversion Rate: {stats['ACTUAL_CONVERSION_RATE']:.1%}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Summary failed: {str(e)}\")\n",
        "\n",
        "print(\"\\n‚úÖ Simple scoring view is ready to use!\")\n",
        "print(\"\\nüí° You can now run queries like:\")\n",
        "print(\"\"\"\n",
        "-- Get high probability clients:\n",
        "SELECT * FROM CLIENT_CONVERSION_SCORES \n",
        "WHERE CONVERSION_PROBABILITY > 0.7\n",
        "ORDER BY CONVERSION_PROBABILITY DESC;\n",
        "\n",
        "-- Compare by segment:\n",
        "SELECT \n",
        "    AGE_SEGMENT,\n",
        "    AVG(CONVERSION_PROBABILITY) as AVG_PROB,\n",
        "    COUNT(*) as CLIENT_COUNT\n",
        "FROM CLIENT_CONVERSION_SCORES\n",
        "GROUP BY AGE_SEGMENT\n",
        "ORDER BY AVG_PROB DESC;\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Proper Model Registry inference\n",
        "print(\"üéØ Using Model Registry with correct syntax...\")\n",
        "\n",
        "# First, let's check what input format the model expects\n",
        "print(\"\\nüìã Checking model requirements...\")\n",
        "\n",
        "# Create a view that properly formats data for the model\n",
        "model_input_view = \"\"\"\n",
        "CREATE OR REPLACE VIEW MODEL_INPUT_DATA AS\n",
        "SELECT \n",
        "    CLIENT_ID,\n",
        "    LIFECYCLE_STAGE,\n",
        "    AGE_SEGMENT,\n",
        "    BUSINESS_PRIORITY_SCORE,\n",
        "    CONVERSION_TARGET,\n",
        "    \n",
        "    -- Create an OBJECT with all features for the model\n",
        "    OBJECT_CONSTRUCT(\n",
        "        'TOTAL_EVENTS_30D', TOTAL_EVENTS_30D,\n",
        "        'WEB_VISITS_30D', WEB_VISITS_30D,\n",
        "        'EMAIL_OPENS_30D', EMAIL_OPENS_30D,\n",
        "        'EMAIL_CLICKS_30D', EMAIL_CLICKS_30D,\n",
        "        'ENGAGEMENT_FREQUENCY_30D', ENGAGEMENT_FREQUENCY_30D,\n",
        "        'ENGAGEMENT_SCORE_30D', ENGAGEMENT_SCORE_30D,\n",
        "        'DAYS_SINCE_LAST_ACTIVITY', DAYS_SINCE_LAST_ACTIVITY,\n",
        "        'AGE', AGE,\n",
        "        'ANNUAL_INCOME', ANNUAL_INCOME,\n",
        "        'CURRENT_401K_BALANCE', CURRENT_401K_BALANCE,\n",
        "        'YEARS_TO_RETIREMENT', YEARS_TO_RETIREMENT,\n",
        "        'TOTAL_ASSETS_UNDER_MANAGEMENT', TOTAL_ASSETS_UNDER_MANAGEMENT,\n",
        "        'CLIENT_TENURE_MONTHS', CLIENT_TENURE_MONTHS,\n",
        "        'INCOME_TO_AGE_RATIO', INCOME_TO_AGE_RATIO,\n",
        "        'ASSETS_TO_INCOME_RATIO', ASSETS_TO_INCOME_RATIO,\n",
        "        'RETIREMENT_READINESS_SCORE', RETIREMENT_READINESS_SCORE,\n",
        "        'WEALTH_GROWTH_POTENTIAL', WEALTH_GROWTH_POTENTIAL,\n",
        "        'SERVICE_TIER_NUMERIC', SERVICE_TIER_NUMERIC,\n",
        "        'RISK_TOLERANCE_NUMERIC', RISK_TOLERANCE_NUMERIC,\n",
        "        'TOTAL_LIFETIME_EVENTS', TOTAL_LIFETIME_EVENTS,\n",
        "        'EDUCATION_ENGAGEMENT', EDUCATION_ENGAGEMENT,\n",
        "        'ADVISOR_MEETINGS_TOTAL', ADVISOR_MEETINGS_TOTAL,\n",
        "        'WEB_PREFERENCE_RATIO', WEB_PREFERENCE_RATIO,\n",
        "        'EMAIL_PREFERENCE_RATIO', EMAIL_PREFERENCE_RATIO,\n",
        "        'MOBILE_ADOPTION_SCORE', MOBILE_ADOPTION_SCORE,\n",
        "        'LIFETIME_ENGAGEMENT_FREQUENCY', LIFETIME_ENGAGEMENT_FREQUENCY,\n",
        "        'BUSINESS_PRIORITY_SCORE', BUSINESS_PRIORITY_SCORE,\n",
        "        'LIFECYCLE_STAGE_ENCODED', CASE LIFECYCLE_STAGE\n",
        "            WHEN 'New' THEN 1\n",
        "            WHEN 'Onboarding' THEN 2\n",
        "            WHEN 'Active' THEN 3\n",
        "            WHEN 'Engaged' THEN 4\n",
        "            WHEN 'At Risk' THEN 5\n",
        "            WHEN 'Dormant' THEN 6\n",
        "            ELSE 0\n",
        "        END,\n",
        "        'AGE_SEGMENT_ENCODED', CASE AGE_SEGMENT\n",
        "            WHEN 'Young Professional' THEN 1\n",
        "            WHEN 'Early Career' THEN 2\n",
        "            WHEN 'Peak Earning' THEN 3\n",
        "            WHEN 'Pre-Retirement' THEN 4\n",
        "            WHEN 'Retirement' THEN 5\n",
        "            ELSE 0\n",
        "        END,\n",
        "        'TENURE_SEGMENT_ENCODED', CASE TENURE_SEGMENT\n",
        "            WHEN 'New' THEN 1\n",
        "            WHEN 'Developing' THEN 2\n",
        "            WHEN 'Established' THEN 3\n",
        "            WHEN 'Long-term' THEN 4\n",
        "            ELSE 0\n",
        "        END\n",
        "    ) AS FEATURES\n",
        "FROM FEATURE_STORE\n",
        "\"\"\"\n",
        "\n",
        "session.sql(model_input_view).collect()\n",
        "print(\"‚úÖ Model input view created\")\n",
        "\n",
        "# Now create prediction view using Model Registry with OBJECT input\n",
        "print(\"\\nüìä Creating Model Registry prediction view...\")\n",
        "\n",
        "registry_view_sql = \"\"\"\n",
        "CREATE OR REPLACE VIEW MODEL_REGISTRY_PREDICTIONS AS\n",
        "SELECT \n",
        "    CLIENT_ID,\n",
        "    LIFECYCLE_STAGE,\n",
        "    AGE_SEGMENT,\n",
        "    BUSINESS_PRIORITY_SCORE,\n",
        "    CONVERSION_TARGET AS ACTUAL_CONVERSION,\n",
        "    MODEL(CONVERSION_PREDICTOR)!predict(FEATURES) AS CONVERSION_PROBABILITY\n",
        "FROM MODEL_INPUT_DATA\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    session.sql(registry_view_sql).collect()\n",
        "    print(\"‚úÖ Model Registry prediction view created!\")\n",
        "    \n",
        "    # Test it\n",
        "    print(\"\\nüß™ Testing Model Registry predictions...\")\n",
        "    test_results = session.sql(\"\"\"\n",
        "        SELECT \n",
        "            CLIENT_ID,\n",
        "            LIFECYCLE_STAGE,\n",
        "            AGE_SEGMENT,\n",
        "            CONVERSION_PROBABILITY,\n",
        "            ACTUAL_CONVERSION\n",
        "        FROM MODEL_REGISTRY_PREDICTIONS\n",
        "        ORDER BY CONVERSION_PROBABILITY DESC\n",
        "        LIMIT 10\n",
        "    \"\"\").collect()\n",
        "    \n",
        "    print(\"\\nTop 10 predictions:\")\n",
        "    print(\"Client ID | Lifecycle  | Age Segment     | Probability | Actual\")\n",
        "    print(\"-\" * 70)\n",
        "    for row in test_results:\n",
        "        print(f\"{row['CLIENT_ID']:<9} | {row['LIFECYCLE_STAGE']:<10} | {row['AGE_SEGMENT']:<15} | {row['CONVERSION_PROBABILITY']:<11.4f} | {row['ACTUAL_CONVERSION']}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error: {str(e)}\")\n",
        "    \n",
        "    # Try alternative: pass features as DataFrame to model\n",
        "    print(\"\\nüîß Trying Python API approach...\")\n",
        "    \n",
        "    try:\n",
        "        from snowflake.ml.registry import Registry\n",
        "        registry = Registry(session=session)\n",
        "        model_version = registry.get_model(\"CONVERSION_PREDICTOR\").version(\"V1\")\n",
        "        \n",
        "        # Get sample data\n",
        "        sample_data = session.sql(f\"\"\"\n",
        "            SELECT {', '.join(all_features)}\n",
        "            FROM FEATURE_STORE\n",
        "            LIMIT 100\n",
        "        \"\"\")\n",
        "        \n",
        "        # Run predictions\n",
        "        predictions = model_version.run(sample_data, function_name=\"predict\")\n",
        "        \n",
        "        # Join with original data for display\n",
        "        original_data = session.sql(\"\"\"\n",
        "            SELECT CLIENT_ID, LIFECYCLE_STAGE, AGE_SEGMENT, CONVERSION_TARGET\n",
        "            FROM FEATURE_STORE\n",
        "            LIMIT 100\n",
        "        \"\"\")\n",
        "        \n",
        "        # Create a table with predictions\n",
        "        predictions.write.mode(\"overwrite\").save_as_table(\"TEMP_PREDICTIONS\")\n",
        "        \n",
        "        # Create final prediction view\n",
        "        final_view = \"\"\"\n",
        "        CREATE OR REPLACE VIEW MODEL_PREDICTIONS_FINAL AS\n",
        "        SELECT \n",
        "            f.CLIENT_ID,\n",
        "            f.LIFECYCLE_STAGE,\n",
        "            f.AGE_SEGMENT,\n",
        "            f.BUSINESS_PRIORITY_SCORE,\n",
        "            p.PREDICTION as CONVERSION_PROBABILITY,\n",
        "            f.CONVERSION_TARGET as ACTUAL_CONVERSION\n",
        "        FROM (\n",
        "            SELECT *, ROW_NUMBER() OVER (ORDER BY CLIENT_ID) as RN\n",
        "            FROM FEATURE_STORE\n",
        "        ) f\n",
        "        JOIN (\n",
        "            SELECT *, ROW_NUMBER() OVER (ORDER BY 1) as RN\n",
        "            FROM TEMP_PREDICTIONS\n",
        "        ) p ON f.RN = p.RN\n",
        "        \"\"\"\n",
        "        \n",
        "        session.sql(final_view).collect()\n",
        "        print(\"‚úÖ Created prediction view using Python API\")\n",
        "        \n",
        "    except Exception as e2:\n",
        "        print(f\"‚ùå Python approach also failed: {str(e2)}\")\n",
        "\n",
        "print(\"\\nüí° Query examples for Model Registry predictions:\")\n",
        "print(\"\"\"\n",
        "-- Get high probability conversions:\n",
        "SELECT * FROM MODEL_REGISTRY_PREDICTIONS \n",
        "WHERE CONVERSION_PROBABILITY > 0.7\n",
        "ORDER BY CONVERSION_PROBABILITY DESC;\n",
        "\n",
        "-- Summary by segment:\n",
        "SELECT \n",
        "    AGE_SEGMENT,\n",
        "    COUNT(*) as CLIENTS,\n",
        "    AVG(CONVERSION_PROBABILITY) as AVG_PROBABILITY,\n",
        "    SUM(CASE WHEN ACTUAL_CONVERSION = 1 THEN 1 ELSE 0 END) as ACTUAL_CONVERSIONS\n",
        "FROM MODEL_REGISTRY_PREDICTIONS\n",
        "GROUP BY AGE_SEGMENT\n",
        "ORDER BY AVG_PROBABILITY DESC;\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Success! Model Registry is working - let's analyze the predictions\n",
        "print(\"üéâ Model Registry predictions are working!\")\n",
        "\n",
        "# Get summary statistics\n",
        "print(\"\\nüìä Prediction Summary Statistics:\")\n",
        "summary_stats = session.sql(\"\"\"\n",
        "    SELECT \n",
        "        COUNT(*) as TOTAL_CLIENTS,\n",
        "        AVG(CONVERSION_PROBABILITY) as AVG_PROBABILITY,\n",
        "        MIN(CONVERSION_PROBABILITY) as MIN_PROBABILITY,\n",
        "        MAX(CONVERSION_PROBABILITY) as MAX_PROBABILITY,\n",
        "        STDDEV(CONVERSION_PROBABILITY) as STDDEV_PROBABILITY,\n",
        "        MEDIAN(CONVERSION_PROBABILITY) as MEDIAN_PROBABILITY\n",
        "    FROM CLIENT_CONVERSION_PREDICTIONS\n",
        "\"\"\").collect()[0]\n",
        "\n",
        "print(f\"Total Clients: {summary_stats['TOTAL_CLIENTS']:,}\")\n",
        "print(f\"Average Probability: {summary_stats['AVG_PROBABILITY']:.4f}\")\n",
        "print(f\"Min Probability: {summary_stats['MIN_PROBABILITY']:.4f}\")\n",
        "print(f\"Max Probability: {summary_stats['MAX_PROBABILITY']:.4f}\")\n",
        "print(f\"Std Dev: {summary_stats['STDDEV_PROBABILITY']:.4f}\")\n",
        "print(f\"Median: {summary_stats['MEDIAN_PROBABILITY']:.4f}\")\n",
        "\n",
        "# Analyze predictions by segment\n",
        "print(\"\\nüìä Predictions by Age Segment:\")\n",
        "age_analysis = session.sql(\"\"\"\n",
        "    SELECT \n",
        "        AGE_SEGMENT,\n",
        "        COUNT(*) as CLIENT_COUNT,\n",
        "        AVG(CONVERSION_PROBABILITY) as AVG_PROBABILITY,\n",
        "        SUM(CASE WHEN ACTUAL_CONVERSION = 1 THEN 1 ELSE 0 END) as ACTUAL_CONVERSIONS,\n",
        "        AVG(CASE WHEN ACTUAL_CONVERSION = 1 THEN 1.0 ELSE 0.0 END) as ACTUAL_RATE\n",
        "    FROM CLIENT_CONVERSION_PREDICTIONS\n",
        "    GROUP BY AGE_SEGMENT\n",
        "    ORDER BY AVG_PROBABILITY DESC\n",
        "\"\"\").collect()\n",
        "\n",
        "print(\"\\nAge Segment          | Clients | Avg Prob | Actual Rate\")\n",
        "print(\"-\" * 55)\n",
        "for row in age_analysis:\n",
        "    print(f\"{row['AGE_SEGMENT']:<20} | {row['CLIENT_COUNT']:>7,} | {row['AVG_PROBABILITY']:>8.3f} | {row['ACTUAL_RATE']:>11.1%}\")\n",
        "\n",
        "# Analyze by lifecycle stage\n",
        "print(\"\\nüìä Predictions by Lifecycle Stage:\")\n",
        "lifecycle_analysis = session.sql(\"\"\"\n",
        "    SELECT \n",
        "        LIFECYCLE_STAGE,\n",
        "        COUNT(*) as CLIENT_COUNT,\n",
        "        AVG(CONVERSION_PROBABILITY) as AVG_PROBABILITY,\n",
        "        AVG(BUSINESS_PRIORITY_SCORE) as AVG_PRIORITY\n",
        "    FROM CLIENT_CONVERSION_PREDICTIONS\n",
        "    GROUP BY LIFECYCLE_STAGE\n",
        "    ORDER BY AVG_PROBABILITY DESC\n",
        "\"\"\").collect()\n",
        "\n",
        "print(\"\\nLifecycle   | Clients | Avg Prob | Avg Priority\")\n",
        "print(\"-\" * 50)\n",
        "for row in lifecycle_analysis:\n",
        "    print(f\"{row['LIFECYCLE_STAGE']:<11} | {row['CLIENT_COUNT']:>7,} | {row['AVG_PROBABILITY']:>8.3f} | {row['AVG_PRIORITY']:>12.1f}\")\n",
        "\n",
        "# Model performance evaluation\n",
        "print(\"\\nüìä Model Performance Evaluation:\")\n",
        "performance = session.sql(\"\"\"\n",
        "    WITH probability_bands AS (\n",
        "        SELECT \n",
        "            CASE \n",
        "                WHEN CONVERSION_PROBABILITY >= 0.8 THEN '0.8-1.0'\n",
        "                WHEN CONVERSION_PROBABILITY >= 0.6 THEN '0.6-0.8'\n",
        "                WHEN CONVERSION_PROBABILITY >= 0.4 THEN '0.4-0.6'\n",
        "                WHEN CONVERSION_PROBABILITY >= 0.2 THEN '0.2-0.4'\n",
        "                ELSE '0.0-0.2'\n",
        "            END as PROB_BAND,\n",
        "            ACTUAL_CONVERSION\n",
        "        FROM CLIENT_CONVERSION_PREDICTIONS\n",
        "    )\n",
        "    SELECT \n",
        "        PROB_BAND,\n",
        "        COUNT(*) as CLIENT_COUNT,\n",
        "        SUM(ACTUAL_CONVERSION) as CONVERSIONS,\n",
        "        AVG(ACTUAL_CONVERSION) as CONVERSION_RATE\n",
        "    FROM probability_bands\n",
        "    GROUP BY PROB_BAND\n",
        "    ORDER BY PROB_BAND DESC\n",
        "\"\"\").collect()\n",
        "\n",
        "print(\"\\nProb Band | Clients | Conversions | Conversion Rate\")\n",
        "print(\"-\" * 55)\n",
        "for row in performance:\n",
        "    print(f\"{row['PROB_BAND']:<9} | {row['CLIENT_COUNT']:>7,} | {row['CONVERSIONS']:>11,} | {row['CONVERSION_RATE']:>15.1%}\")\n",
        "\n",
        "# Top conversion candidates\n",
        "print(\"\\nüéØ Top 10 Conversion Candidates:\")\n",
        "top_candidates = session.sql(\"\"\"\n",
        "    SELECT \n",
        "        CLIENT_ID,\n",
        "        LIFECYCLE_STAGE,\n",
        "        AGE_SEGMENT,\n",
        "        CONVERSION_PROBABILITY,\n",
        "        BUSINESS_PRIORITY_SCORE\n",
        "    FROM CLIENT_CONVERSION_PREDICTIONS\n",
        "    ORDER BY CONVERSION_PROBABILITY DESC\n",
        "    LIMIT 10\n",
        "\"\"\").collect()\n",
        "\n",
        "print(\"\\nClient ID  | Lifecycle  | Age Segment     | Probability | Priority\")\n",
        "print(\"-\" * 70)\n",
        "for row in top_candidates:\n",
        "    print(f\"{row['CLIENT_ID']:<10} | {row['LIFECYCLE_STAGE']:<10} | {row['AGE_SEGMENT']:<15} | {row['CONVERSION_PROBABILITY']:>11.4f} | {row['BUSINESS_PRIORITY_SCORE']:>8.1f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Model Registry inference is working successfully!\")\n",
        "print(\"üìà Your XGBoost model is making predictions on all 50,000 clients\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
