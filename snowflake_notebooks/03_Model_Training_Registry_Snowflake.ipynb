{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training & Registry with Snowflake ML\n",
        "## Financial Services ML Pipeline - Native Snowflake Implementation\n",
        "\n",
        "This notebook demonstrates model training and registration using Snowflake's Model Registry for financial services ML.\n",
        "\n",
        "## What We'll Build\n",
        "- **Classification Models**: Conversion prediction, churn prediction\n",
        "- **Multi-class Classification**: Next best action recommendation\n",
        "- **Model Comparison**: XGBoost, Random Forest, and LogisticRegression\n",
        "- **Model Registry**: Version control and lifecycle management\n",
        "- **Performance Evaluation**: Comprehensive model assessment\n",
        "\n",
        "## Snowflake ML Features Used\n",
        "- **Snowpark ML**: Native ML training within Snowflake\n",
        "- **Model Registry**: Centralized model management and versioning\n",
        "- **Cross-validation**: Robust model evaluation\n",
        "- **Feature Engineering**: Automated preprocessing pipelines\n",
        "- **Model Deployment**: Seamless deployment for inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries for ML training\n",
        "import snowflake.snowpark as snowpark\n",
        "from snowflake.snowpark import Session\n",
        "from snowflake.snowpark.functions import *\n",
        "from snowflake.ml.modeling.xgboost import XGBClassifier\n",
        "from snowflake.ml.modeling.ensemble import RandomForestClassifier\n",
        "from snowflake.ml.modeling.linear_model import LogisticRegression\n",
        "from snowflake.ml.modeling.preprocessing import StandardScaler, LabelEncoder\n",
        "from snowflake.ml.modeling.model_selection import train_test_split, cross_validate\n",
        "from snowflake.ml.modeling.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from snowflake.ml.registry import Registry\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "# Get active session\n",
        "session = snowpark.session._get_active_session()\n",
        "\n",
        "print(f\"ü§ñ Snowflake ML Model Training Pipeline\")\n",
        "print(f\"Database: {session.get_current_database()}\")\n",
        "print(f\"Schema: {session.get_current_schema()}\")\n",
        "print(f\"Warehouse: {session.get_current_warehouse()}\")\n",
        "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Verify feature store availability\n",
        "fs_count = session.sql(\"SELECT COUNT(*) as count FROM feature_store\").collect()[0]['COUNT']\n",
        "feature_count = session.sql(\"SELECT COUNT(*) as feature_count FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'FEATURE_STORE'\").collect()[0]['FEATURE_COUNT']\n",
        "\n",
        "print(f\"\\nFeature Store Ready:\")\n",
        "print(f\"üìä Training Records: {fs_count:,}\")\n",
        "print(f\"üîß Available Features: {feature_count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Data Preparation & Feature Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare training data with feature selection\n",
        "print(\"üìã Preparing training data and selecting features...\")\n",
        "\n",
        "# Define feature sets for different models\n",
        "numeric_features = [\n",
        "    'total_events_30d', 'web_visits_30d', 'email_opens_30d', 'email_clicks_30d',\n",
        "    'engagement_frequency_30d', 'engagement_score_30d', 'days_since_last_activity',\n",
        "    'age', 'annual_income', 'current_401k_balance', 'years_to_retirement',\n",
        "    'total_assets_under_management', 'client_tenure_months',\n",
        "    'income_to_age_ratio', 'assets_to_income_ratio', 'retirement_readiness_score',\n",
        "    'wealth_growth_potential', 'premium_client_indicator',\n",
        "    'service_tier_numeric', 'risk_tolerance_numeric', 'investment_experience_numeric',\n",
        "    'total_lifetime_events', 'education_engagement', 'advisor_meetings_total',\n",
        "    'web_preference_ratio', 'email_preference_ratio', 'mobile_adoption_score',\n",
        "    'lifetime_engagement_frequency', 'business_priority_score'\n",
        "]\n",
        "\n",
        "categorical_features = [\n",
        "    'lifecycle_stage', 'age_segment', 'tenure_segment'\n",
        "]\n",
        "\n",
        "# Load and prepare training data\n",
        "training_data_sql = f\"\"\"\n",
        "SELECT \n",
        "    client_id,\n",
        "    {', '.join(numeric_features)},\n",
        "    {', '.join(categorical_features)},\n",
        "    conversion_target,\n",
        "    churn_target,\n",
        "    next_best_action\n",
        "FROM feature_store\n",
        "WHERE conversion_target IS NOT NULL \n",
        "  AND churn_target IS NOT NULL\n",
        "  AND next_best_action IS NOT NULL\n",
        "\"\"\"\n",
        "\n",
        "# Load data as Snowpark DataFrame\n",
        "training_df = session.sql(training_data_sql)\n",
        "\n",
        "print(f\"‚úÖ Training data prepared\")\n",
        "print(f\"   üî¢ Numeric features: {len(numeric_features)}\")\n",
        "print(f\"   üìù Categorical features: {len(categorical_features)}\")\n",
        "\n",
        "# Show data distribution for targets\n",
        "print(\"\\nüìä Target variable distributions:\")\n",
        "target_stats = session.sql(\"\"\"\n",
        "    SELECT \n",
        "        SUM(conversion_target) as conversion_positives,\n",
        "        COUNT(*) - SUM(conversion_target) as conversion_negatives,\n",
        "        SUM(churn_target) as churn_positives,\n",
        "        COUNT(*) - SUM(churn_target) as churn_negatives,\n",
        "        COUNT(DISTINCT next_best_action) as action_classes,\n",
        "        COUNT(*) as total_samples\n",
        "    FROM feature_store\n",
        "    WHERE conversion_target IS NOT NULL\n",
        "\"\"\").collect()[0]\n",
        "\n",
        "print(f\"Conversion: {target_stats['CONVERSION_POSITIVES']} positive, {target_stats['CONVERSION_NEGATIVES']} negative\")\n",
        "print(f\"Churn: {target_stats['CHURN_POSITIVES']} positive, {target_stats['CHURN_NEGATIVES']} negative\")\n",
        "print(f\"Next Action: {target_stats['ACTION_CLASSES']} classes, {target_stats['TOTAL_SAMPLES']} total samples\")\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nüîç Data quality check:\")\n",
        "session.sql(f\"\"\"\n",
        "    SELECT \n",
        "        COUNT(*) as total_records,\n",
        "        COUNT(CASE WHEN {' IS NULL OR '.join(numeric_features[:5])} IS NULL THEN 1 END) as missing_key_features\n",
        "    FROM feature_store\n",
        "    WHERE conversion_target IS NOT NULL\n",
        "\"\"\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Train Conversion Prediction Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train conversion prediction models\n",
        "print(\"üéØ Training conversion prediction models...\")\n",
        "\n",
        "# Prepare data for conversion prediction\n",
        "conversion_features = numeric_features + categorical_features\n",
        "conversion_df = training_df.select(*conversion_features, 'conversion_target')\n",
        "\n",
        "# Handle missing values and prepare for training\n",
        "conversion_clean_sql = \"\"\"\n",
        "CREATE OR REPLACE TEMPORARY TABLE conversion_training AS\n",
        "SELECT *\n",
        "FROM (\"\"\" + training_data_sql + \"\"\")\n",
        "WHERE \"\"\" + \" AND \".join([f\"{feat} IS NOT NULL\" for feat in numeric_features[:10]]) + \"\"\"\n",
        "\"\"\"\n",
        "\n",
        "session.sql(conversion_clean_sql).collect()\n",
        "conversion_df_clean = session.table(\"conversion_training\")\n",
        "\n",
        "print(\"‚úÖ Data cleaned for conversion prediction\")\n",
        "\n",
        "# Split data for training\n",
        "X_cols = [col for col in conversion_features if col != 'client_id']\n",
        "y_col = 'conversion_target'\n",
        "\n",
        "# Create train/test split using Snowpark ML\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    conversion_df_clean.select(*X_cols),\n",
        "    conversion_df_clean.select(y_col),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Train/test split completed\")\n",
        "\n",
        "# Train XGBoost model for conversion prediction\n",
        "print(\"\\nüå≤ Training XGBoost for conversion prediction...\")\n",
        "\n",
        "xgb_conversion = XGBClassifier(\n",
        "    max_depth=6,\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "xgb_conversion.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "conversion_predictions = xgb_conversion.predict(X_test)\n",
        "conversion_probabilities = xgb_conversion.predict_proba(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "conv_accuracy = accuracy_score(y_test, conversion_predictions)\n",
        "conv_precision = precision_score(y_test, conversion_predictions)\n",
        "conv_recall = recall_score(y_test, conversion_predictions) \n",
        "conv_f1 = f1_score(y_test, conversion_predictions)\n",
        "\n",
        "print(f\"‚úÖ XGBoost Conversion Model Results:\")\n",
        "print(f\"   üìä Accuracy: {conv_accuracy:.4f}\")\n",
        "print(f\"   üìä Precision: {conv_precision:.4f}\")\n",
        "print(f\"   üìä Recall: {conv_recall:.4f}\")\n",
        "print(f\"   üìä F1-Score: {conv_f1:.4f}\")\n",
        "\n",
        "# Train Random Forest for comparison\n",
        "print(\"\\nüå≥ Training Random Forest for conversion prediction...\")\n",
        "\n",
        "rf_conversion = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_conversion.fit(X_train, y_train)\n",
        "rf_predictions = rf_conversion.predict(X_test)\n",
        "\n",
        "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
        "rf_precision = precision_score(y_test, rf_predictions)\n",
        "rf_recall = recall_score(y_test, rf_predictions)\n",
        "rf_f1 = f1_score(y_test, rf_predictions)\n",
        "\n",
        "print(f\"‚úÖ Random Forest Conversion Model Results:\")\n",
        "print(f\"   üìä Accuracy: {rf_accuracy:.4f}\")\n",
        "print(f\"   üìä Precision: {rf_precision:.4f}\")\n",
        "print(f\"   üìä Recall: {rf_recall:.4f}\")\n",
        "print(f\"   üìä F1-Score: {rf_f1:.4f}\")\n",
        "\n",
        "# Select best model\n",
        "if conv_f1 >= rf_f1:\n",
        "    best_conversion_model = xgb_conversion\n",
        "    best_conv_score = conv_f1\n",
        "    best_conv_name = \"XGBoost\"\n",
        "else:\n",
        "    best_conversion_model = rf_conversion\n",
        "    best_conv_score = rf_f1\n",
        "    best_conv_name = \"RandomForest\"\n",
        "\n",
        "print(f\"\\nüèÜ Best Conversion Model: {best_conv_name} (F1: {best_conv_score:.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
