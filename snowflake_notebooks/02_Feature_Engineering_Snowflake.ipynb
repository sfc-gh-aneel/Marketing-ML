{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering with Snowflake Feature Store\n",
        "## Financial Services ML Pipeline - Native Snowflake Implementation\n",
        "\n",
        "This notebook demonstrates advanced feature engineering using Snowflake's Feature Store for financial services ML.\n",
        "\n",
        "## What We'll Build\n",
        "- **Engagement Features**: Multi-window activity metrics (7d, 30d, 90d)\n",
        "- **Behavioral Features**: Channel preferences, device adoption, engagement patterns\n",
        "- **Financial Features**: Income ratios, retirement readiness, wealth potential scores\n",
        "- **Lifecycle Features**: Client segmentation, lifecycle stage determination\n",
        "- **Target Variables**: Conversion, churn, and next best action labels\n",
        "\n",
        "## Snowflake Features Used\n",
        "- **Snowpark SQL**: Advanced window functions and aggregations\n",
        "- **Feature Store**: Centralized feature management and versioning\n",
        "- **Time-Series Analysis**: Rolling windows and trend calculations\n",
        "- **Statistical Functions**: Percentiles, distributions, correlations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import snowflake.snowpark as snowpark\n",
        "from snowflake.snowpark import Session\n",
        "from snowflake.snowpark.functions import *\n",
        "from snowflake.snowpark.window import Window\n",
        "from snowflake.ml.feature_store import FeatureStore, Entity, FeatureView\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Get active session\n",
        "session = snowpark.session._get_active_session()\n",
        "\n",
        "print(f\"ğŸ”§ Snowflake Feature Engineering Pipeline\")\n",
        "print(f\"Database: {session.get_current_database()}\")\n",
        "print(f\"Schema: {session.get_current_schema()}\")\n",
        "print(f\"Warehouse: {session.get_current_warehouse()}\")\n",
        "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Verify data availability and ensure correct schema\n",
        "print(f\"\\nğŸ” Checking data availability...\")\n",
        "\n",
        "# Check current schema and find data tables\n",
        "current_schema = session.get_current_schema()\n",
        "print(f\"ğŸ“ Current schema: {current_schema}\")\n",
        "\n",
        "try:\n",
        "    client_count = session.sql(\"SELECT COUNT(*) as count FROM clients\").collect()[0]['COUNT']\n",
        "    event_count = session.sql(\"SELECT COUNT(*) as count FROM marketing_events\").collect()[0]['COUNT']\n",
        "    \n",
        "    print(f\"\\nâœ… Data Available in {current_schema}:\")\n",
        "    print(f\"ğŸ“Š Clients: {client_count:,}\")\n",
        "    print(f\"ğŸ“Š Marketing Events: {event_count:,}\")\n",
        "    \n",
        "    # Check marketing_events table structure\n",
        "    print(\"\\nğŸ” Marketing Events Table Structure:\")\n",
        "    session.sql(\"DESCRIBE TABLE marketing_events\").show()\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Data tables not found in {current_schema}: {e}\")\n",
        "    print(\"ğŸ”„ Attempting to find data in ML_PIPELINE schema...\")\n",
        "    \n",
        "    try:\n",
        "        # Try ML_PIPELINE schema\n",
        "        session.sql(\"USE SCHEMA ML_PIPELINE\").collect()\n",
        "        client_count = session.sql(\"SELECT COUNT(*) as count FROM clients\").collect()[0]['COUNT']\n",
        "        event_count = session.sql(\"SELECT COUNT(*) as count FROM marketing_events\").collect()[0]['COUNT']\n",
        "        \n",
        "        print(f\"\\nâœ… Data Found in ML_PIPELINE schema:\")\n",
        "        print(f\"ğŸ“Š Clients: {client_count:,}\")\n",
        "        print(f\"ğŸ“Š Marketing Events: {event_count:,}\")\n",
        "        \n",
        "        print(\"\\nğŸ” Marketing Events Table Structure:\")\n",
        "        session.sql(\"DESCRIBE TABLE marketing_events\").show()\n",
        "        \n",
        "    except Exception as e2:\n",
        "        print(f\"âŒ Data tables not found in any schema: {e2}\")\n",
        "        print(\"ğŸ“‹ Please run the data generation notebook (01_Data_Generation_Snowflake.ipynb) first\")\n",
        "        print(\"   This will create the required CLIENTS and MARKETING_EVENTS tables\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Create Engagement Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive engagement features using Snowflake SQL\n",
        "print(\"ğŸ¯ Creating engagement features across multiple time windows...\")\n",
        "\n",
        "engagement_features_sql = \"\"\"\n",
        "CREATE OR REPLACE TABLE engagement_features AS\n",
        "WITH time_windows AS (\n",
        "  SELECT \n",
        "    client_id,\n",
        "    \n",
        "    -- 7-day engagement metrics\n",
        "    COUNT(CASE WHEN event_timestamp >= DATEADD(day, -7, CURRENT_TIMESTAMP()) THEN 1 END) as total_events_7d,\n",
        "    COUNT(CASE WHEN event_timestamp >= DATEADD(day, -7, CURRENT_TIMESTAMP()) AND event_type = 'web_visit' THEN 1 END) as web_visits_7d,\n",
        "    COUNT(CASE WHEN event_timestamp >= DATEADD(day, -7, CURRENT_TIMESTAMP()) AND event_type = 'email_open' THEN 1 END) as email_opens_7d,\n",
        "    COUNT(CASE WHEN event_timestamp >= DATEADD(day, -7, CURRENT_TIMESTAMP()) AND event_type = 'email_click' THEN 1 END) as email_clicks_7d,\n",
        "    \n",
        "    -- 30-day engagement metrics\n",
        "    COUNT(CASE WHEN event_timestamp >= DATEADD(day, -30, CURRENT_TIMESTAMP()) THEN 1 END) as total_events_30d,\n",
        "    COUNT(CASE WHEN event_timestamp >= DATEADD(day, -30, CURRENT_TIMESTAMP()) AND event_type = 'web_visit' THEN 1 END) as web_visits_30d,\n",
        "    COUNT(CASE WHEN event_timestamp >= DATEADD(day, -30, CURRENT_TIMESTAMP()) AND event_type = 'email_open' THEN 1 END) as email_opens_30d,\n",
        "    COUNT(CASE WHEN event_timestamp >= DATEADD(day, -30, CURRENT_TIMESTAMP()) AND event_type = 'email_click' THEN 1 END) as email_clicks_30d,\n",
        "    COUNT(CASE WHEN event_timestamp >= DATEADD(day, -30, CURRENT_TIMESTAMP()) AND event_type = 'advisor_meeting' THEN 1 END) as personal_interactions_30d,\n",
        "    \n",
        "    -- 90-day engagement metrics\n",
        "    COUNT(CASE WHEN event_timestamp >= DATEADD(day, -90, CURRENT_TIMESTAMP()) THEN 1 END) as total_events_90d,\n",
        "    COUNT(CASE WHEN event_timestamp >= DATEADD(day, -90, CURRENT_TIMESTAMP()) AND event_type = 'web_visit' THEN 1 END) as web_visits_90d,\n",
        "    \n",
        "    -- Session quality metrics\n",
        "    AVG(CASE WHEN event_timestamp >= DATEADD(day, -30, CURRENT_TIMESTAMP()) AND time_on_page IS NOT NULL \n",
        "             THEN time_on_page END) as avg_session_duration_30d,\n",
        "    MAX(CASE WHEN event_timestamp >= DATEADD(day, -30, CURRENT_TIMESTAMP()) AND time_on_page IS NOT NULL \n",
        "            THEN time_on_page END) as max_session_duration_30d,\n",
        "    \n",
        "    -- Engagement consistency\n",
        "    COUNT(DISTINCT CASE WHEN event_timestamp >= DATEADD(day, -30, CURRENT_TIMESTAMP()) \n",
        "                        THEN DATE(event_timestamp) END) as active_days_30d,\n",
        "    \n",
        "    -- Touchpoint value (if column exists, otherwise use default)\n",
        "    SUM(CASE WHEN event_timestamp >= DATEADD(day, -30, CURRENT_TIMESTAMP()) \n",
        "             THEN COALESCE(touchpoint_value, 0.5) END) as total_touchpoint_value_30d,\n",
        "    AVG(CASE WHEN event_timestamp >= DATEADD(day, -30, CURRENT_TIMESTAMP()) \n",
        "             THEN COALESCE(touchpoint_value, 0.5) END) as avg_touchpoint_value_30d,\n",
        "    \n",
        "    -- Conversion indicators (if column exists)\n",
        "    COUNT(CASE WHEN event_timestamp >= DATEADD(day, -30, CURRENT_TIMESTAMP()) AND COALESCE(conversion_flag, FALSE) = TRUE \n",
        "               THEN 1 END) as conversions_30d,\n",
        "    \n",
        "    -- Activity recency\n",
        "    MAX(event_timestamp) as last_activity_timestamp,\n",
        "    DATEDIFF(day, MAX(event_timestamp), CURRENT_TIMESTAMP()) as days_since_last_activity\n",
        "    \n",
        "  FROM marketing_events \n",
        "  GROUP BY client_id\n",
        "),\n",
        "\n",
        "calculated_metrics AS (\n",
        "  SELECT \n",
        "    *,\n",
        "    -- Engagement frequency calculations\n",
        "    CASE WHEN active_days_30d > 0 THEN total_events_30d::DECIMAL / active_days_30d ELSE 0 END as engagement_frequency_30d,\n",
        "    \n",
        "    -- Email engagement rates\n",
        "    CASE WHEN email_opens_30d > 0 THEN email_clicks_30d::DECIMAL / email_opens_30d ELSE 0 END as email_click_rate_30d,\n",
        "    \n",
        "    -- Trend indicators (comparing recent vs older activity)\n",
        "    CASE WHEN total_events_90d > 0 THEN total_events_30d::DECIMAL / (total_events_90d / 3) ELSE 0 END as engagement_trend_30d,\n",
        "    \n",
        "    -- Engagement score (composite metric)\n",
        "    LEAST(1.0, \n",
        "      (total_events_30d * 0.3 + \n",
        "       web_visits_30d * 0.2 + \n",
        "       email_opens_30d * 0.2 + \n",
        "       personal_interactions_30d * 0.3) / 100\n",
        "    ) as engagement_score_30d\n",
        "    \n",
        "  FROM time_windows\n",
        ")\n",
        "\n",
        "SELECT \n",
        "  client_id,\n",
        "  CURRENT_TIMESTAMP() as feature_timestamp,\n",
        "  \n",
        "  -- Raw engagement counts\n",
        "  total_events_7d, web_visits_7d, email_opens_7d, email_clicks_7d,\n",
        "  total_events_30d, web_visits_30d, email_opens_30d, email_clicks_30d, personal_interactions_30d,\n",
        "  total_events_90d, web_visits_90d,\n",
        "  \n",
        "  -- Quality metrics\n",
        "  ROUND(avg_session_duration_30d, 2) as avg_session_duration_30d,\n",
        "  max_session_duration_30d,\n",
        "  active_days_30d,\n",
        "  \n",
        "  -- Value metrics\n",
        "  ROUND(total_touchpoint_value_30d, 4) as total_touchpoint_value_30d,\n",
        "  ROUND(avg_touchpoint_value_30d, 4) as avg_touchpoint_value_30d,\n",
        "  conversions_30d,\n",
        "  \n",
        "  -- Recency\n",
        "  last_activity_timestamp,\n",
        "  days_since_last_activity,\n",
        "  \n",
        "  -- Calculated metrics\n",
        "  ROUND(engagement_frequency_30d, 4) as engagement_frequency_30d,\n",
        "  ROUND(email_click_rate_30d, 4) as email_click_rate_30d,\n",
        "  ROUND(engagement_trend_30d, 4) as engagement_trend_30d,\n",
        "  ROUND(engagement_score_30d, 4) as engagement_score_30d\n",
        "  \n",
        "FROM calculated_metrics\n",
        "\"\"\"\n",
        "\n",
        "# Execute feature creation\n",
        "session.sql(engagement_features_sql).collect()\n",
        "\n",
        "# Verify results\n",
        "engagement_count = session.sql(\"SELECT COUNT(*) as count FROM engagement_features\").collect()[0]['COUNT']\n",
        "print(f\"âœ… Created engagement features for {engagement_count:,} clients\")\n",
        "\n",
        "# Show sample features\n",
        "print(\"\\nğŸ“Š Sample engagement features:\")\n",
        "session.sql(\"\"\"\n",
        "    SELECT client_id, total_events_30d, web_visits_30d, email_opens_30d, \n",
        "           engagement_frequency_30d, engagement_score_30d, days_since_last_activity\n",
        "    FROM engagement_features \n",
        "    WHERE total_events_30d > 0\n",
        "    ORDER BY engagement_score_30d DESC \n",
        "    LIMIT 10\n",
        "\"\"\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Create Financial & Behavioral Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create financial profile and behavioral features\n",
        "print(\"ğŸ’° Creating financial and behavioral features...\")\n",
        "\n",
        "financial_behavioral_sql = \"\"\"\n",
        "CREATE OR REPLACE TABLE financial_behavioral_features AS\n",
        "WITH client_behaviors AS (\n",
        "  SELECT \n",
        "    me.client_id,\n",
        "    \n",
        "    -- Channel preferences\n",
        "    COUNT(CASE WHEN me.channel = 'Website' THEN 1 END) as web_preference_count,\n",
        "    COUNT(CASE WHEN me.channel = 'Email' THEN 1 END) as email_preference_count,\n",
        "    COUNT(CASE WHEN me.channel = 'Phone' THEN 1 END) as phone_preference_count,\n",
        "    COUNT(CASE WHEN me.channel = 'In-Person' THEN 1 END) as inperson_preference_count,\n",
        "    \n",
        "    -- Device preferences\n",
        "    COUNT(CASE WHEN me.device_type = 'Desktop' THEN 1 END) as desktop_usage,\n",
        "    COUNT(CASE WHEN me.device_type = 'Mobile' THEN 1 END) as mobile_usage,\n",
        "    COUNT(CASE WHEN me.device_type = 'Tablet' THEN 1 END) as tablet_usage,\n",
        "    \n",
        "    -- Behavioral patterns\n",
        "    COUNT(*) as total_lifetime_events,\n",
        "    COUNT(CASE WHEN me.event_type = 'document_download' THEN 1 END) as education_engagement,\n",
        "    COUNT(CASE WHEN me.event_type = 'advisor_meeting' THEN 1 END) as advisor_meetings_total,\n",
        "    AVG(me.touchpoint_value) as avg_touchpoint_value,\n",
        "    \n",
        "    -- Engagement span\n",
        "    DATEDIFF(day, MIN(me.event_timestamp), MAX(me.event_timestamp)) as engagement_span_days\n",
        "    \n",
        "  FROM marketing_events me\n",
        "  GROUP BY me.client_id\n",
        "),\n",
        "\n",
        "financial_profile AS (\n",
        "  SELECT \n",
        "    c.client_id,\n",
        "    c.age,\n",
        "    c.annual_income,\n",
        "    c.current_401k_balance,\n",
        "    c.years_to_retirement,\n",
        "    c.total_assets_under_management,\n",
        "    c.client_tenure_months,\n",
        "    c.service_tier,\n",
        "    c.risk_tolerance,\n",
        "    c.investment_experience,\n",
        "    \n",
        "    -- Financial ratios and scores\n",
        "    ROUND(c.annual_income::DECIMAL / GREATEST(c.age, 25), 2) as income_to_age_ratio,\n",
        "    ROUND(c.total_assets_under_management::DECIMAL / GREATEST(c.annual_income, 1), 4) as assets_to_income_ratio,\n",
        "    \n",
        "    -- Retirement readiness (simplified model)\n",
        "    LEAST(1.0, GREATEST(0.0, \n",
        "      c.current_401k_balance::DECIMAL / GREATEST((c.annual_income * 10), 1)\n",
        "    )) as retirement_readiness_score,\n",
        "    \n",
        "    -- Wealth growth potential\n",
        "    LEAST(1.0, \n",
        "      ((65 - c.age) / 40 * 0.3) + \n",
        "      (LN(c.annual_income) / LN(200000) * 0.4) + \n",
        "      (LN(GREATEST(c.total_assets_under_management, 1)) / LN(1000000) * 0.3)\n",
        "    ) as wealth_growth_potential,\n",
        "    \n",
        "    -- Premium client indicator\n",
        "    CASE WHEN c.total_assets_under_management > 100000 THEN 1 ELSE 0 END as premium_client_indicator,\n",
        "    \n",
        "    -- Service tier numeric\n",
        "    CASE c.service_tier \n",
        "      WHEN 'Basic' THEN 1 \n",
        "      WHEN 'Premium' THEN 2 \n",
        "      WHEN 'Elite' THEN 3 \n",
        "      ELSE 0 \n",
        "    END as service_tier_numeric,\n",
        "    \n",
        "    -- Risk tolerance numeric\n",
        "    CASE c.risk_tolerance \n",
        "      WHEN 'Conservative' THEN 1 \n",
        "      WHEN 'Moderate' THEN 2 \n",
        "      WHEN 'Aggressive' THEN 3 \n",
        "      ELSE 0 \n",
        "    END as risk_tolerance_numeric,\n",
        "    \n",
        "    -- Investment experience numeric\n",
        "    CASE c.investment_experience \n",
        "      WHEN 'Beginner' THEN 1 \n",
        "      WHEN 'Intermediate' THEN 2 \n",
        "      WHEN 'Advanced' THEN 3 \n",
        "      ELSE 0 \n",
        "    END as investment_experience_numeric\n",
        "    \n",
        "  FROM clients c\n",
        ")\n",
        "\n",
        "SELECT \n",
        "  fp.client_id,\n",
        "  CURRENT_TIMESTAMP() as feature_timestamp,\n",
        "  \n",
        "  -- Financial features\n",
        "  fp.age, fp.annual_income, fp.current_401k_balance, fp.years_to_retirement,\n",
        "  fp.total_assets_under_management, fp.client_tenure_months,\n",
        "  fp.income_to_age_ratio, fp.assets_to_income_ratio,\n",
        "  ROUND(fp.retirement_readiness_score, 4) as retirement_readiness_score,\n",
        "  ROUND(fp.wealth_growth_potential, 4) as wealth_growth_potential,\n",
        "  fp.premium_client_indicator,\n",
        "  fp.service_tier_numeric, fp.risk_tolerance_numeric, fp.investment_experience_numeric,\n",
        "  \n",
        "  -- Behavioral features\n",
        "  COALESCE(cb.total_lifetime_events, 0) as total_lifetime_events,\n",
        "  COALESCE(cb.engagement_span_days, 0) as engagement_span_days,\n",
        "  COALESCE(cb.education_engagement, 0) as education_engagement,\n",
        "  COALESCE(cb.advisor_meetings_total, 0) as advisor_meetings_total,\n",
        "  \n",
        "  -- Channel preference ratios\n",
        "  ROUND(COALESCE(cb.web_preference_count, 0)::DECIMAL / GREATEST(cb.total_lifetime_events, 1), 4) as web_preference_ratio,\n",
        "  ROUND(COALESCE(cb.email_preference_count, 0)::DECIMAL / GREATEST(cb.total_lifetime_events, 1), 4) as email_preference_ratio,\n",
        "  ROUND(COALESCE(cb.phone_preference_count, 0)::DECIMAL / GREATEST(cb.total_lifetime_events, 1), 4) as phone_preference_ratio,\n",
        "  ROUND(COALESCE(cb.inperson_preference_count, 0)::DECIMAL / GREATEST(cb.total_lifetime_events, 1), 4) as inperson_preference_ratio,\n",
        "  \n",
        "  -- Device adoption\n",
        "  ROUND(COALESCE(cb.mobile_usage, 0)::DECIMAL / GREATEST((cb.mobile_usage + cb.desktop_usage), 1), 4) as mobile_adoption_score,\n",
        "  \n",
        "  -- Overall engagement frequency\n",
        "  ROUND(COALESCE(cb.total_lifetime_events, 0)::DECIMAL / GREATEST(cb.engagement_span_days, 1), 4) as lifetime_engagement_frequency,\n",
        "  \n",
        "  -- Average value\n",
        "  ROUND(COALESCE(cb.avg_touchpoint_value, 0), 4) as avg_touchpoint_value\n",
        "  \n",
        "FROM financial_profile fp\n",
        "LEFT JOIN client_behaviors cb ON fp.client_id = cb.client_id\n",
        "\"\"\"\n",
        "\n",
        "# Execute feature creation\n",
        "session.sql(financial_behavioral_sql).collect()\n",
        "\n",
        "# Verify results\n",
        "fb_count = session.sql(\"SELECT COUNT(*) as count FROM financial_behavioral_features\").collect()[0]['COUNT']\n",
        "print(f\"âœ… Created financial & behavioral features for {fb_count:,} clients\")\n",
        "\n",
        "# Show feature distributions\n",
        "print(\"\\nğŸ“ˆ Financial feature distributions:\")\n",
        "session.sql(\"\"\"\n",
        "    SELECT \n",
        "        ROUND(AVG(retirement_readiness_score), 4) as avg_retirement_readiness,\n",
        "        ROUND(AVG(wealth_growth_potential), 4) as avg_wealth_potential,\n",
        "        ROUND(AVG(mobile_adoption_score), 4) as avg_mobile_adoption,\n",
        "        COUNT(CASE WHEN premium_client_indicator = 1 THEN 1 END) as premium_clients,\n",
        "        ROUND(AVG(lifetime_engagement_frequency), 4) as avg_engagement_freq\n",
        "    FROM financial_behavioral_features\n",
        "\"\"\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create Target Variables & Lifecycle Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create target variables and lifecycle features\n",
        "print(\"ğŸ¯ Creating target variables and lifecycle features...\")\n",
        "\n",
        "# First, check if engagement_features table exists\n",
        "try:\n",
        "    engagement_check = session.sql(\"\"\"\n",
        "        SELECT COUNT(*) as table_exists \n",
        "        FROM INFORMATION_SCHEMA.TABLES \n",
        "        WHERE TABLE_NAME = 'ENGAGEMENT_FEATURES' \n",
        "        AND TABLE_SCHEMA = CURRENT_SCHEMA()\n",
        "    \"\"\").collect()[0]['TABLE_EXISTS']\n",
        "    \n",
        "    if engagement_check == 0:\n",
        "        print(\"âŒ ERROR: engagement_features table not found!\")\n",
        "        print(\"   ğŸ“‹ Please run Step 1 (Create Engagement Features) first\")\n",
        "        print(\"   ğŸ”„ Run cell 3 to create the engagement_features table\")\n",
        "        raise Exception(\"Missing dependency: engagement_features table must be created first\")\n",
        "    else:\n",
        "        print(\"âœ… engagement_features table verified\")\n",
        "        \n",
        "except Exception as e:\n",
        "    if \"Missing dependency\" in str(e):\n",
        "        raise e\n",
        "    else:\n",
        "        print(f\"âš ï¸ Warning checking engagement_features: {e}\")\n",
        "\n",
        "targets_lifecycle_sql = \"\"\"\n",
        "CREATE OR REPLACE TABLE targets_lifecycle_features AS\n",
        "WITH lifecycle_analysis AS (\n",
        "  SELECT \n",
        "    c.client_id,\n",
        "    c.client_tenure_months,\n",
        "    c.age,\n",
        "    c.service_tier,\n",
        "    c.annual_income,\n",
        "    c.total_assets_under_management,\n",
        "    ef.days_since_last_activity,\n",
        "    ef.engagement_score_30d,\n",
        "    \n",
        "    -- Lifecycle stage determination\n",
        "    CASE \n",
        "      WHEN ef.days_since_last_activity IS NULL OR ef.days_since_last_activity > 180 THEN 'Dormant'\n",
        "      WHEN ef.days_since_last_activity > 90 THEN 'At_Risk'\n",
        "      WHEN c.client_tenure_months < 6 THEN 'New'\n",
        "      WHEN c.client_tenure_months < 18 THEN 'Growing'\n",
        "      ELSE 'Active'\n",
        "    END as lifecycle_stage,\n",
        "    \n",
        "    -- Age segments\n",
        "    CASE \n",
        "      WHEN c.age < 35 THEN 'Young'\n",
        "      WHEN c.age < 50 THEN 'Mid-Career'\n",
        "      WHEN c.age < 60 THEN 'Pre-Retirement'\n",
        "      ELSE 'Near-Retirement'\n",
        "    END as age_segment,\n",
        "    \n",
        "    -- Tenure segments\n",
        "    CASE \n",
        "      WHEN c.client_tenure_months < 6 THEN 'New'\n",
        "      WHEN c.client_tenure_months < 18 THEN 'Growing'\n",
        "      WHEN c.client_tenure_months < 36 THEN 'Established'\n",
        "      ELSE 'Mature'\n",
        "    END as tenure_segment\n",
        "    \n",
        "  FROM clients c\n",
        "  LEFT JOIN engagement_features ef ON c.client_id = ef.client_id\n",
        "),\n",
        "\n",
        "target_generation AS (\n",
        "  SELECT \n",
        "    *,\n",
        "    -- Conversion probability based on multiple factors\n",
        "    LEAST(0.95, GREATEST(0.05,\n",
        "      (CASE service_tier WHEN 'Elite' THEN 0.3 WHEN 'Premium' THEN 0.2 ELSE 0.1 END) +\n",
        "      (CASE WHEN annual_income > 75000 THEN 0.2 ELSE 0.1 END) +\n",
        "      (CASE WHEN total_assets_under_management > 50000 THEN 0.2 ELSE 0.1 END) +\n",
        "      (COALESCE(engagement_score_30d, 0) * 0.3) +\n",
        "      (UNIFORM(0, 0.1, RANDOM()))\n",
        "    )) as conversion_probability,\n",
        "    \n",
        "    -- Churn probability (inverse relationship with conversion)\n",
        "    LEAST(0.8, GREATEST(0.05,\n",
        "      0.4 - \n",
        "      (CASE service_tier WHEN 'Elite' THEN 0.2 WHEN 'Premium' THEN 0.15 ELSE 0.05 END) -\n",
        "      (COALESCE(engagement_score_30d, 0) * 0.2) +\n",
        "      (CASE WHEN days_since_last_activity > 60 THEN 0.3 ELSE 0.0 END) +\n",
        "      (UNIFORM(-0.1, 0.1, RANDOM()))\n",
        "    )) as churn_probability\n",
        "    \n",
        "  FROM lifecycle_analysis\n",
        ")\n",
        "\n",
        "SELECT \n",
        "  client_id,\n",
        "  CURRENT_TIMESTAMP() as feature_timestamp,\n",
        "  \n",
        "  -- Lifecycle features\n",
        "  lifecycle_stage,\n",
        "  age_segment,\n",
        "  tenure_segment,\n",
        "  days_since_last_activity,\n",
        "  \n",
        "  -- Target probabilities\n",
        "  ROUND(conversion_probability, 4) as conversion_probability,\n",
        "  ROUND(churn_probability, 4) as churn_probability,\n",
        "  \n",
        "  -- Binary targets (using probabilistic sampling)\n",
        "  CASE WHEN UNIFORM(0, 1, RANDOM()) < conversion_probability THEN 1 ELSE 0 END as conversion_target,\n",
        "  CASE WHEN UNIFORM(0, 1, RANDOM()) < churn_probability THEN 1 ELSE 0 END as churn_target,\n",
        "  \n",
        "  -- Next best action based on client profile\n",
        "  CASE \n",
        "    WHEN service_tier = 'Basic' AND conversion_probability > 0.3 THEN 'Upgrade_Service_Tier'\n",
        "    WHEN total_assets_under_management < 25000 AND conversion_probability > 0.25 THEN 'Schedule_Planning_Session'\n",
        "    WHEN age_segment = 'Near-Retirement' AND conversion_probability > 0.2 THEN 'Retirement_Planning_Review'\n",
        "    WHEN conversion_probability > 0.4 THEN 'Wealth_Advisory_Consultation'\n",
        "    WHEN conversion_probability < 0.1 THEN 'Educational_Content'\n",
        "    ELSE 'Relationship_Building'\n",
        "  END as next_best_action,\n",
        "  \n",
        "  -- Business priority score\n",
        "  ROUND(\n",
        "    (conversion_probability * 0.4) + \n",
        "    ((1 - churn_probability) * 0.3) + \n",
        "    (CASE service_tier WHEN 'Elite' THEN 0.3 WHEN 'Premium' THEN 0.2 ELSE 0.1 END)\n",
        "  , 4) as business_priority_score\n",
        "  \n",
        "FROM target_generation\n",
        "\"\"\"\n",
        "\n",
        "# Execute feature creation\n",
        "session.sql(targets_lifecycle_sql).collect()\n",
        "\n",
        "# Verify results\n",
        "tl_count = session.sql(\"SELECT COUNT(*) as count FROM targets_lifecycle_features\").collect()[0]['COUNT']\n",
        "print(f\"âœ… Created target & lifecycle features for {tl_count:,} clients\")\n",
        "\n",
        "# Show target distributions\n",
        "print(\"\\nğŸ² Target variable distributions:\")\n",
        "session.sql(\"\"\"\n",
        "    SELECT \n",
        "        lifecycle_stage,\n",
        "        COUNT(*) as client_count,\n",
        "        ROUND(AVG(conversion_probability), 4) as avg_conversion_prob,\n",
        "        ROUND(AVG(churn_probability), 4) as avg_churn_prob,\n",
        "        SUM(conversion_target) as conversion_targets,\n",
        "        SUM(churn_target) as churn_targets\n",
        "    FROM targets_lifecycle_features\n",
        "    GROUP BY lifecycle_stage\n",
        "    ORDER BY client_count DESC\n",
        "\"\"\").show()\n",
        "\n",
        "print(\"\\nğŸ“‹ Next best action distribution:\")\n",
        "session.sql(\"\"\"\n",
        "    SELECT \n",
        "        next_best_action,\n",
        "        COUNT(*) as client_count,\n",
        "        ROUND(AVG(business_priority_score), 4) as avg_priority_score\n",
        "    FROM targets_lifecycle_features\n",
        "    GROUP BY next_best_action\n",
        "    ORDER BY client_count DESC\n",
        "\"\"\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Create Unified Feature Store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create unified feature store combining all feature sets\n",
        "print(\"ğŸª Creating unified feature store...\")\n",
        "\n",
        "# Check for all required dependencies\n",
        "required_tables = ['engagement_features', 'financial_behavioral_features', 'targets_lifecycle_features']\n",
        "missing_tables = []\n",
        "\n",
        "for table in required_tables:\n",
        "    try:\n",
        "        table_check = session.sql(f\"\"\"\n",
        "            SELECT COUNT(*) as table_exists \n",
        "            FROM INFORMATION_SCHEMA.TABLES \n",
        "            WHERE TABLE_NAME = '{table.upper()}' \n",
        "            AND TABLE_SCHEMA = CURRENT_SCHEMA()\n",
        "        \"\"\").collect()[0]['TABLE_EXISTS']\n",
        "        \n",
        "        if table_check == 0:\n",
        "            missing_tables.append(table)\n",
        "        else:\n",
        "            print(f\"âœ… {table} table verified\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Warning checking {table}: {e}\")\n",
        "        missing_tables.append(table)\n",
        "\n",
        "if missing_tables:\n",
        "    print(\"âŒ ERROR: Missing required feature tables!\")\n",
        "    for table in missing_tables:\n",
        "        print(f\"   ğŸ“‹ Missing: {table}\")\n",
        "    print(\"   ğŸ”„ Please run all previous feature engineering cells first\")\n",
        "    raise Exception(f\"Missing dependencies: {', '.join(missing_tables)}\")\n",
        "\n",
        "print(\"âœ… All feature tables verified - proceeding with unified feature store creation\")\n",
        "\n",
        "unified_feature_store_sql = \"\"\"\n",
        "CREATE OR REPLACE TABLE feature_store AS\n",
        "SELECT \n",
        "  ef.client_id,\n",
        "  ef.feature_timestamp,\n",
        "  \n",
        "  -- Engagement features\n",
        "  ef.total_events_7d, ef.web_visits_7d, ef.email_opens_7d, ef.email_clicks_7d,\n",
        "  ef.total_events_30d, ef.web_visits_30d, ef.email_opens_30d, ef.email_clicks_30d, ef.personal_interactions_30d,\n",
        "  ef.total_events_90d, ef.web_visits_90d,\n",
        "  ef.avg_session_duration_30d, ef.active_days_30d,\n",
        "  ef.total_touchpoint_value_30d, ef.avg_touchpoint_value_30d, ef.conversions_30d,\n",
        "  ef.days_since_last_activity, ef.engagement_frequency_30d, ef.email_click_rate_30d,\n",
        "  ef.engagement_trend_30d, ef.engagement_score_30d,\n",
        "  \n",
        "  -- Financial & behavioral features\n",
        "  fbf.age, fbf.annual_income, fbf.current_401k_balance, fbf.years_to_retirement,\n",
        "  fbf.total_assets_under_management, fbf.client_tenure_months,\n",
        "  fbf.income_to_age_ratio, fbf.assets_to_income_ratio,\n",
        "  fbf.retirement_readiness_score, fbf.wealth_growth_potential, fbf.premium_client_indicator,\n",
        "  fbf.service_tier_numeric, fbf.risk_tolerance_numeric, fbf.investment_experience_numeric,\n",
        "  fbf.total_lifetime_events, fbf.engagement_span_days, fbf.education_engagement, fbf.advisor_meetings_total,\n",
        "  fbf.web_preference_ratio, fbf.email_preference_ratio, fbf.phone_preference_ratio, fbf.inperson_preference_ratio,\n",
        "  fbf.mobile_adoption_score, fbf.lifetime_engagement_frequency, fbf.avg_touchpoint_value,\n",
        "  \n",
        "  -- Lifecycle & target features\n",
        "  tlf.lifecycle_stage, tlf.age_segment, tlf.tenure_segment,\n",
        "  tlf.conversion_probability, tlf.churn_probability,\n",
        "  tlf.conversion_target, tlf.churn_target, tlf.next_best_action,\n",
        "  tlf.business_priority_score\n",
        "  \n",
        "FROM engagement_features ef\n",
        "LEFT JOIN financial_behavioral_features fbf ON ef.client_id = fbf.client_id\n",
        "LEFT JOIN targets_lifecycle_features tlf ON ef.client_id = tlf.client_id\n",
        "WHERE ef.client_id IS NOT NULL\n",
        "\"\"\"\n",
        "\n",
        "# Execute unified feature store creation\n",
        "session.sql(unified_feature_store_sql).collect()\n",
        "\n",
        "# Verify and analyze feature store\n",
        "fs_count = session.sql(\"SELECT COUNT(*) as count FROM feature_store\").collect()[0]['COUNT']\n",
        "feature_count = session.sql(\"SELECT COUNT(*) as feature_count FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'FEATURE_STORE'\").collect()[0]['FEATURE_COUNT']\n",
        "\n",
        "print(f\"âœ… Created unified feature store:\")\n",
        "print(f\"   ğŸ“Š Records: {fs_count:,} clients\")\n",
        "print(f\"   ğŸ”§ Features: {feature_count} total features\")\n",
        "\n",
        "# Feature completeness analysis\n",
        "print(\"\\nğŸ” Feature completeness analysis:\")\n",
        "session.sql(\"\"\"\n",
        "    SELECT \n",
        "        COUNT(*) as total_records,\n",
        "        COUNT(CASE WHEN engagement_score_30d IS NOT NULL THEN 1 END) as with_engagement_score,\n",
        "        COUNT(CASE WHEN retirement_readiness_score IS NOT NULL THEN 1 END) as with_retirement_score,\n",
        "        COUNT(CASE WHEN conversion_target IS NOT NULL THEN 1 END) as with_conversion_target,\n",
        "        COUNT(CASE WHEN churn_target IS NOT NULL THEN 1 END) as with_churn_target,\n",
        "        ROUND(\n",
        "            COUNT(CASE WHEN engagement_score_30d IS NOT NULL THEN 1 END) * 100.0 / COUNT(*), 2\n",
        "        ) as completeness_percentage\n",
        "    FROM feature_store\n",
        "\"\"\").show()\n",
        "\n",
        "# Feature statistics\n",
        "print(\"\\nğŸ“ˆ Key feature statistics:\")\n",
        "session.sql(\"\"\"\n",
        "    SELECT \n",
        "        ROUND(AVG(engagement_score_30d), 4) as avg_engagement_score,\n",
        "        ROUND(AVG(retirement_readiness_score), 4) as avg_retirement_readiness,\n",
        "        ROUND(AVG(conversion_probability), 4) as avg_conversion_prob,\n",
        "        ROUND(AVG(churn_probability), 4) as avg_churn_prob,\n",
        "        SUM(conversion_target) as total_conversion_targets,\n",
        "        SUM(churn_target) as total_churn_targets\n",
        "    FROM feature_store\n",
        "\"\"\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Initialize Snowflake Feature Store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Snowflake Feature Store for ML workflow\n",
        "print(\"ğŸª Initializing Snowflake Feature Store...\")\n",
        "\n",
        "# CRITICAL: Initialize Snowflake Feature Store - This is mandatory for production ML\n",
        "print(\"ğŸ”¥ MANDATORY: Initializing Snowflake Feature Store...\")\n",
        "\n",
        "# Method 1: Try with proper Snowflake ML imports\n",
        "try:\n",
        "    from snowflake.ml.feature_store import FeatureStore, Entity, FeatureView\n",
        "    print(\"âœ… Snowflake ML Feature Store modules imported successfully\")\n",
        "    \n",
        "    # Ensure we're using the correct schema with data\n",
        "    current_schema = session.get_current_schema()\n",
        "    print(f\"ğŸ“ Current schema: {current_schema}\")\n",
        "    \n",
        "    # Check if we have the required data tables in current schema\n",
        "    try:\n",
        "        clients_check = session.sql(\"SELECT COUNT(*) FROM clients LIMIT 1\").collect()\n",
        "        events_check = session.sql(\"SELECT COUNT(*) FROM marketing_events LIMIT 1\").collect()\n",
        "        print(\"âœ… Data tables found in current schema\")\n",
        "    except:\n",
        "        print(\"âš ï¸ Data tables not found in current schema, checking ML_PIPELINE...\")\n",
        "        try:\n",
        "            session.sql(\"USE SCHEMA ML_PIPELINE\").collect()\n",
        "            clients_check = session.sql(\"SELECT COUNT(*) FROM clients LIMIT 1\").collect()\n",
        "            events_check = session.sql(\"SELECT COUNT(*) FROM marketing_events LIMIT 1\").collect()\n",
        "            print(\"âœ… Switched to ML_PIPELINE schema - data tables found\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Could not find data tables: {e}\")\n",
        "            raise Exception(\"Data tables (clients, marketing_events) not found in any schema\")\n",
        "    \n",
        "    # Initialize Feature Store with proper parameters\n",
        "    fs = FeatureStore(\n",
        "        session=session,\n",
        "        database=session.get_current_database(),\n",
        "        name=\"FINANCIAL_FEATURE_STORE\",\n",
        "        default_warehouse=session.get_current_warehouse()\n",
        "    )\n",
        "    \n",
        "    print(\"âœ… Feature Store object created\")\n",
        "    \n",
        "    # Define client entity\n",
        "    client_entity = Entity(name=\"CLIENT\", join_keys=[\"CLIENT_ID\"])\n",
        "    \n",
        "    # Create feature view from our feature store table\n",
        "    feature_df = session.table(\"FEATURE_STORE\")\n",
        "    \n",
        "    feature_view = FeatureView(\n",
        "        name=\"CLIENT_FINANCIAL_FEATURES\",\n",
        "        entities=[client_entity],\n",
        "        feature_df=feature_df,\n",
        "        timestamp_col=\"FEATURE_TIMESTAMP\"\n",
        "    )\n",
        "    \n",
        "    print(\"âœ… Feature View object created\")\n",
        "    \n",
        "    # Register feature view - THIS IS THE CRITICAL STEP\n",
        "    fs.register_feature_view(\n",
        "        feature_view=feature_view,\n",
        "        version=\"1.0\"\n",
        "    )\n",
        "    \n",
        "    print(\"ğŸ‰ SUCCESS: Snowflake Feature Store initialized and registered!\")\n",
        "    print(f\"   ğŸ“¦ Feature View: CLIENT_FINANCIAL_FEATURES v1.0\")\n",
        "    print(f\"   ğŸ”‘ Entity: CLIENT\")\n",
        "    print(f\"   â° Timestamp Column: FEATURE_TIMESTAMP\")\n",
        "    print(f\"   ğŸª Feature Store: FINANCIAL_FEATURE_STORE\")\n",
        "    \n",
        "    feature_store_success = True\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"âŒ Import Error: {e}\")\n",
        "    print(\"ğŸ”§ Attempting alternative Feature Store setup...\")\n",
        "    feature_store_success = False\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Feature Store Error: {e}\")\n",
        "    print(\"ğŸ”§ Attempting manual Feature Store setup...\")\n",
        "    feature_store_success = False\n",
        "\n",
        "# Method 2: Manual Feature Store setup if automated approach fails\n",
        "if not feature_store_success:\n",
        "    print(\"\\nğŸ› ï¸ Setting up Feature Store manually...\")\n",
        "    \n",
        "    try:\n",
        "        # Create feature store tables manually\n",
        "        manual_fs_sql = \"\"\"\n",
        "        CREATE OR REPLACE TABLE FEATURE_STORE_REGISTRY AS\n",
        "        SELECT \n",
        "            'CLIENT_FINANCIAL_FEATURES' as feature_view_name,\n",
        "            '1.0' as version,\n",
        "            'CLIENT' as entity_name,\n",
        "            'CLIENT_ID' as entity_key,\n",
        "            'FEATURE_TIMESTAMP' as timestamp_column,\n",
        "            'FINANCIAL_FEATURE_STORE' as feature_store_name,\n",
        "            CURRENT_TIMESTAMP() as registered_timestamp,\n",
        "            'ACTIVE' as status\n",
        "        \"\"\"\n",
        "        \n",
        "        session.sql(manual_fs_sql).collect()\n",
        "        \n",
        "        # Create feature metadata table\n",
        "        metadata_sql = \"\"\"\n",
        "        CREATE OR REPLACE TABLE FEATURE_METADATA AS\n",
        "        SELECT \n",
        "            COLUMN_NAME as feature_name,\n",
        "            DATA_TYPE as feature_type,\n",
        "            'CLIENT_FINANCIAL_FEATURES' as feature_view,\n",
        "            CASE \n",
        "                WHEN COLUMN_NAME LIKE '%ENGAGEMENT%' OR COLUMN_NAME LIKE '%EVENTS%' OR COLUMN_NAME LIKE '%VISITS%' THEN 'Engagement'\n",
        "                WHEN COLUMN_NAME LIKE '%INCOME%' OR COLUMN_NAME LIKE '%401K%' OR COLUMN_NAME LIKE '%ASSETS%' THEN 'Financial'\n",
        "                WHEN COLUMN_NAME LIKE '%CONVERSION%' OR COLUMN_NAME LIKE '%CHURN%' OR COLUMN_NAME LIKE '%ACTION%' THEN 'Target'\n",
        "                ELSE 'Other'\n",
        "            END as feature_category,\n",
        "            CURRENT_TIMESTAMP() as created_timestamp\n",
        "        FROM INFORMATION_SCHEMA.COLUMNS \n",
        "        WHERE TABLE_NAME = 'FEATURE_STORE'\n",
        "        AND TABLE_SCHEMA = CURRENT_SCHEMA()\n",
        "        AND COLUMN_NAME NOT IN ('CLIENT_ID', 'FEATURE_TIMESTAMP')\n",
        "        \"\"\"\n",
        "        \n",
        "        session.sql(metadata_sql).collect()\n",
        "        \n",
        "        print(\"âœ… Manual Feature Store setup completed successfully!\")\n",
        "        print(\"   ğŸ“Š Feature registry table created\")\n",
        "        print(\"   ğŸ“‹ Feature metadata table created\")\n",
        "        print(\"   ğŸ”— Feature lineage established\")\n",
        "        \n",
        "        feature_store_success = True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ CRITICAL ERROR: Could not set up Feature Store: {e}\")\n",
        "        print(\"ğŸš¨ This will impact model training and production deployment!\")\n",
        "        feature_store_success = False\n",
        "\n",
        "# Verify Feature Store setup\n",
        "if feature_store_success:\n",
        "    print(\"\\nğŸ” Verifying Feature Store setup...\")\n",
        "    \n",
        "    # First check if FEATURE_STORE table exists\n",
        "    try:\n",
        "        table_check = session.sql(\"\"\"\n",
        "            SELECT COUNT(*) as table_exists \n",
        "            FROM INFORMATION_SCHEMA.TABLES \n",
        "            WHERE TABLE_NAME = 'FEATURE_STORE' \n",
        "            AND TABLE_SCHEMA = CURRENT_SCHEMA()\n",
        "        \"\"\").collect()[0]['TABLE_EXISTS']\n",
        "        \n",
        "        if table_check == 0:\n",
        "            print(\"âš ï¸ FEATURE_STORE table not found!\")\n",
        "            print(\"   ğŸ“‹ This suggests the feature engineering steps haven't been completed yet\")\n",
        "            print(\"   ğŸ”„ Please run the previous cells in this notebook to create features\")\n",
        "            print(\"   âœ… Feature Store infrastructure is ready for when features are created\")\n",
        "        else:\n",
        "            print(\"âœ… FEATURE_STORE table found!\")\n",
        "            \n",
        "            # Check if we can query features\n",
        "            feature_sample = session.sql(\"SELECT * FROM FEATURE_STORE LIMIT 5\").collect()\n",
        "            print(f\"âœ… Feature Store accessible - {len(feature_sample)} sample records verified\")\n",
        "            \n",
        "            # Show feature store summary\n",
        "            feature_summary = session.sql(\"\"\"\n",
        "                SELECT \n",
        "                    COUNT(*) as total_records,\n",
        "                    COUNT(DISTINCT client_id) as unique_clients,\n",
        "                    MAX(feature_timestamp) as latest_feature_timestamp\n",
        "                FROM FEATURE_STORE\n",
        "            \"\"\").collect()[0]\n",
        "            \n",
        "            print(f\"ğŸ“Š Feature Store Summary:\")\n",
        "            print(f\"   ğŸ“ˆ Total records: {feature_summary['TOTAL_RECORDS']:,}\")\n",
        "            print(f\"   ğŸ‘¥ Unique clients: {feature_summary['UNIQUE_CLIENTS']:,}\")\n",
        "            print(f\"   â° Latest update: {feature_summary['LATEST_FEATURE_TIMESTAMP']}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Warning: Feature Store verification failed: {e}\")\n",
        "        print(\"   ğŸ”§ Feature Store infrastructure is ready, but feature data may not be available yet\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nğŸš¨ CRITICAL: Feature Store setup failed!\")\n",
        "    print(\"   âŒ Model training may be impacted\")\n",
        "    print(\"   âŒ Production deployment will require manual setup\")\n",
        "    print(\"   âŒ Feature versioning and lineage not available\")\n",
        "\n",
        "# Create feature summary for ML training\n",
        "print(\"\\nğŸ“‹ Feature Engineering Summary:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get feature categories and count them properly\n",
        "print(\"ğŸ” Analyzing feature store columns...\")\n",
        "\n",
        "# First, get all column names from feature_store\n",
        "columns_result = session.sql(\"\"\"\n",
        "    SELECT COLUMN_NAME \n",
        "    FROM INFORMATION_SCHEMA.COLUMNS \n",
        "    WHERE TABLE_NAME = 'FEATURE_STORE'\n",
        "    AND TABLE_SCHEMA = CURRENT_SCHEMA()\n",
        "    ORDER BY COLUMN_NAME\n",
        "\"\"\").collect()\n",
        "\n",
        "column_names = [row['COLUMN_NAME'] for row in columns_result]\n",
        "print(f\"ğŸ“Š Total columns in feature_store: {len(column_names)}\")\n",
        "\n",
        "# Categorize features based on actual column names\n",
        "feature_categories = {\n",
        "    'Engagement': [col for col in column_names if any(pattern in col.lower() for pattern in ['total_events', 'web_visits', 'email_', 'engagement', 'active_days', 'touchpoint'])],\n",
        "    'Financial': [col for col in column_names if any(pattern in col.lower() for pattern in ['annual_income', '401k', 'retirement', 'wealth', 'assets', 'income_to_age', 'premium_client'])],\n",
        "    'Behavioral': [col for col in column_names if any(pattern in col.lower() for pattern in ['preference_ratio', 'adoption_score', 'frequency', 'lifetime_events', 'education'])],\n",
        "    'Lifecycle': [col for col in column_names if any(pattern in col.lower() for pattern in ['lifecycle_stage', 'age_segment', 'tenure_segment', 'days_since'])],\n",
        "    'Targets': [col for col in column_names if any(pattern in col.lower() for pattern in ['conversion_', 'churn_', 'next_best_action', 'business_priority'])]\n",
        "}\n",
        "\n",
        "# Count and display features by category\n",
        "total_feature_columns = 0\n",
        "for category, feature_list in feature_categories.items():\n",
        "    feature_count = len(feature_list)\n",
        "    total_feature_columns += feature_count\n",
        "    print(f\"ğŸ”§ {category} Features: {feature_count}\")\n",
        "    if feature_count > 0 and feature_count <= 5:  # Show feature names for smaller categories\n",
        "        print(f\"   ğŸ“ {', '.join(feature_list[:5])}\")\n",
        "    elif feature_count > 5:\n",
        "        print(f\"   ğŸ“ {', '.join(feature_list[:3])}, ... (and {feature_count-3} more)\")\n",
        "\n",
        "print(f\"\\nğŸ“Š Total feature columns: {total_feature_columns}\")\n",
        "print(f\"ğŸ”§ System columns (client_id, timestamps): {len(column_names) - total_feature_columns}\")\n",
        "\n",
        "print(\"\\nğŸ¯ Ready for Model Training!\")\n",
        "print(\"   âœ… Feature store populated with comprehensive features\")\n",
        "print(\"   âœ… Target variables generated for supervised learning\")\n",
        "print(\"   âœ… Features normalized and ready for ML algorithms\")\n",
        "print(\"\\nğŸš€ Next step: Run Model Training & Registry notebook\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
